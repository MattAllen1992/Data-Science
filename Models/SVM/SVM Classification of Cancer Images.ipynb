{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification of Cancer Images (Basic SVM)\n",
    "Sources:\n",
    "* [Datacamp](https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python)\n",
    "* [Stackabuse](https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/)\n",
    "### Overview\n",
    "SVM (Support Vector Machines):\n",
    "* Classifier model which allows you to split data using hyperplanes\n",
    "* Hyperplanes are boundaries between data points which are optimized to create the biggest possible margin between the plane itself and data points either side\n",
    "* Support vectors are the closest data points to the hyperplane, these vectors have a larger impact on maximising the margin between the hyperplane and the data points\n",
    "* Kernels are the underlying algorithm/method used in your SVM model to solve the classification problem, they allow transformations of the data points from an original dimension to a higher dimension in order to more easily split the data into classes\n",
    "* For example, if data cannot be linearly split (e.g. you need to draw a ring around the data instead of a straight line), then a certain kernel can transform your data points into a linearly separable distribution to solve this issue\n",
    "* Common kernels include linear (line), polynomial (curve) and radial basis function (complex) and each kernel has custom parameters which need to be set and tuned to optimize your model (e.g. polynomial has the parameter d (for dimension) which allows you to define how many kinks the curve has in it)\n",
    "\n",
    "![SVM](Images/Kernels.png)\n",
    "\n",
    "### Breast Cancer Images\n",
    "Here, we will use an SVM model to determine whether or not breast cancer images show a malignant (cancerous) or benign (safe) cancer cell. The images have been converted into data features and we have a set of targets from previously classified data which will allow us to train and validate our model.\n",
    "\n",
    "### 1. Load and Investigate Data\n",
    "SKLearn provides us with the dataset already split into features and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load library\n",
    "from sklearn import datasets\n",
    "\n",
    "# load data\n",
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "# show feature names\n",
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "       3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "       8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "       3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "       1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show example of data row\n",
    "data.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show target names\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show targets\n",
    "data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimensions of data\n",
    "data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have 30 features which are various properties of the cancer images and 1 target which classes each image as either malignant or benign (0 or 1 respectively). We have 569 images in total and our data is already nicely stored in numpy arrays.\n",
    "\n",
    "### Train and Test Sets\n",
    "Let's split our data into train (70%) and test (30%) sets to allow us to build our model and validate it later without leaking results to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=109) # set seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Fit Model\n",
    "Let's build our model and fit it to our training data. We can then make predictions on our test features and compare with our test targets to assess initial accuracy.\n",
    "\n",
    "We'll begin with the simplest model of SVM, a linear kernel, and see how well this works as a first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "from sklearn import svm\n",
    "\n",
    "# build classifier model\n",
    "clf = svm.SVC(kernel='linear') # linear\n",
    "#clf = SVC(kernel='poly', degree=8) # polynomial with 7 kinks\n",
    "#clf = SVC(kernel='rbf') # gaussian/rbf\n",
    "#clf = SVC(kernel='sigmoid') # sigmoid\n",
    "\n",
    "# fit model to train data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test features\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy\n",
    "Now that we've made our initial predictions, let's compare them with our known results and assess the accuracy of this first attempt (before tuning, optimizing etc.).\n",
    "\n",
    "All of the below scores are very high which could either be because this pre-built dataset is designed to be easy to model (i.e. good, clean data) or that our model is over-fitted even though we are yet to do any tuning. Note that precision and recall are additional accuracy metrics which look at the distribution of true/false positives/negatives between our predicted values and actual results.\n",
    "\n",
    "Our results show us that there are 4 misclassifications from the entire dataset, 2 false positives and 4 false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61   2]\n",
      " [  4 104]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95        63\n",
      "           1       0.98      0.96      0.97       108\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.96      0.97      0.96       171\n",
      "weighted avg       0.97      0.96      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# show accuracy (pred vs. actual)\n",
    "#print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# show precision (true positive / all pred. positives)\n",
    "#print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# show recall (true positive / all real positives)\n",
    "#print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning and Principles\n",
    "Gamma:\n",
    "* Simply, a low version of Gamma leads to underfitting, a high value leads to overfitting\n",
    "* For a low value, the model will only consider local/close data points when determining the hyperplane at each point\n",
    "* Higher gamma values mean that the model looks further, at more data points to decide the hyperplane position\n",
    "* You can tweak this parameter to essentially decide how closely your model chooses to stick to every data point, thinking of data points as magnets for the hyperplane and as gamma increases, so does the strength of each magnet relative to its distance from the hyperplane\n",
    "\n",
    "Regularization:\n",
    "* SVM is a trade off between maximizing margin between classes and minimizing missclassification of classes\n",
    "* In a perfect scenario, you could have a hyperplane with a large margin that separates all classes exactly, however in practice there are often cases where you cannot separate classes without missclassification (i.e. overlapping classes)\n",
    "* The regularization parameter (lambda, or c) lets you set the threshold of missclassification you're willing to accept\n",
    "* A smaller value of c means a more relaxed model where underfitting can occur (i.e. larger hyperplane margins) whilst a higher value of c means a stricter model where overfitting can occur (i.e. narrower margins)\n",
    "* In general, lower (but not tiny) values of c tend to generalise well for SVM\n",
    "* [Regularization Explained](https://datascience.stackexchange.com/questions/4943/intuition-for-the-regularization-parameter-in-svm#:~:text=The%20regularization%20parameter%20(lambda)%20serves,minimizing%20the%20amount%20of%20misclassifications.&text=For%20non%2Dlinear%2Dkernel%20SVM%20the%20idea%20is%20the%20similar)\n",
    "\n",
    "![Regularization](Images/Regularization.png)\n",
    "\n",
    "### Pros and Cons\n",
    "Pros:\n",
    "* Faster prediction than some methods (e.g. Naive Bayes)\n",
    "* Better accuracy than a lot of models when the right kernel is used and the dataset has clear class separation\n",
    "* Less memory than some models due to only using a subset of training data\n",
    "* Works well with high dimensional data (i.e. many features)\n",
    "\n",
    "Cons:\n",
    "* Slows down a lot with larger datasets vs. other models\n",
    "* Doesn't work brilliantly if classes overlap significantly\n",
    "* Sensitive to which kernel method is used and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "To Do:\n",
    "* Visualize data and model (i.e. how is the initial linear kernel working and what does the hyperplane look like?)\n",
    "* Feature selection (correlation, variance, RFE etc.)\n",
    "* Cleaning and checks (any nulls etc.?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
