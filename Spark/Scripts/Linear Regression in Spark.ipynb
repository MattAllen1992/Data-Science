{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "devoted-electronics",
   "metadata": {},
   "source": [
    "# Linear Regression in Spark\n",
    "## 1.) Basic Linear Regression in Spark\n",
    "* Linear regression is a simplistic model for interpreting the relationship between features and labels so that you can then make predictions off new inputs\n",
    "* Spark has a library called MLlib which allows you to perform ML tasks in Spark\n",
    "* The Apache Spark site has solid documentation for all of its libraries, including MLlib and its components (e.g. linear regression, clustering, feature extraction etc.)\n",
    "* Note that the Spark ML code is slightly different from standard Python (e.g. SKLearn), mostly due to the fact that the data needs to be more robustly stored and processed to allow it to run efficiently on distributed big data systems\n",
    "* Below we will cover some of the core elements of linear regression in Spark\n",
    "* Note that the input data here is pre-formatted for us into the Spark format (1 col for labels, 1 col for features) but in practice we will need to transform our raw data to fit this format (we will come onto this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "embedded-elephant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "| -9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "| 0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "| -4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -7.966593841555266|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load libs\n",
    "import findspark\n",
    "\n",
    "# store location of spark files\n",
    "findspark.init('/home/matt/spark-3.0.2-bin-hadoop3.2')\n",
    "\n",
    "# load libs\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# start new session\n",
    "spark = SparkSession.builder.appName('lreg').getOrCreate()\n",
    "\n",
    "# load train data\n",
    "data = spark.read.format('libsvm').load('Data/sample_linear_regression_data.txt')\n",
    "\n",
    "# peek at data\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-bacteria",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "* We can use **randomSplit()** to separate our train and test data randomly\n",
    "* It's important that this is random otherwise we may capture in build ordering/sorting from our raw data\n",
    "* All Spark dataframe objects have this method available to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "genetic-sapphire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0591, 0.9713, -0.949, 2.5666, 1.4099, 1.0595, 0.2065, -0.3038, -1.0501, 1.6254])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly split data into train/test\n",
    "train, test = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# create model instance\n",
    "# you can call these what you like\n",
    "# but they must match your input df\n",
    "# there are many more parameters you can use\n",
    "# shift + tab to see the options available\n",
    "lr = LinearRegression(featuresCol='features',\n",
    "                      labelCol='label',\n",
    "                      predictionCol='prediction')\n",
    "\n",
    "# create model var\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# check co-efficients\n",
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-donna",
   "metadata": {},
   "source": [
    "### Evaluating our Model\n",
    "* Once a model is built in Spark, you can access all of the evaluation metrics you could need\n",
    "* R2, MSE, RMSE, MAE etc. are all available of the model object\n",
    "* You can of course calculate the same metrics for your test data once you've applied your trained model to the test features to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vanilla-alloy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.641994426807195"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check y-intercept\n",
    "lr_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hidden-brunswick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03870866789062066"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get summary of model\n",
    "train_summary = lr_model.summary\n",
    "\n",
    "# show r2 value\n",
    "# use shift + tab to show all options\n",
    "train_summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "closing-model",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.75977379756242"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show MSE\n",
    "train_summary.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "under-gentleman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.791620046000943"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict labels based on test features\n",
    "# test_results is essentially y_pred\n",
    "test_results = lr_model.evaluate(test)\n",
    "\n",
    "# assess scores of test predictions\n",
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-connecticut",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "* The above code showed us how well our labelled test data fitted to our linear model\n",
    "* However, we can also make new predictions using the model and our test features\n",
    "* Then we can assess the prediction score of our model by comparing predicted values to actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adaptive-spotlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract test data without labels\n",
    "unlabelled_data = test.select('features')\n",
    "\n",
    "# show data\n",
    "unlabelled_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bored-speech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|(10,[0,1,2,3,4,5,...|-1.347831113305837|\n",
      "|(10,[0,1,2,3,4,5,...|0.2744128758073392|\n",
      "|(10,[0,1,2,3,4,5,...|3.0104295755815533|\n",
      "+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions using model and unlabelled test data\n",
    "predictions = lr_model.transform(unlabelled_data)\n",
    "\n",
    "# show predictions\n",
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-disorder",
   "metadata": {},
   "source": [
    "## 2.) Evaluation Metrics\n",
    "### Mean Absolute Error (MAE)\n",
    "* Simple absolute value of error\n",
    "* For each instance, calculate error between actual and predicted value\n",
    "* Convert this to absolute value to remove -ve and +ve cancelling out\n",
    "* Average the absolute error across all instances\n",
    "* Tells you how far away from your actual results your predictions are using your model\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "* Slight improvement on the above method\n",
    "* Same process as MAE except that you square the difference between actual and predicted\n",
    "* This penalizes larger errors in your model so that when your predictions are way off, this is highlighted further\n",
    "* The only issue here is that because you've squared your values, you are no longer looking at the raw scales for your values, making the MSE harder to interpret literally\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "* As such, the RMSE improves upon the above method\n",
    "* It does so by finding the square root of the MSE (i.e. finds error, squares it, averages this across all instances and then square roots the final value)\n",
    "* This means that it successfully penalizes larger error predictions whilst also reverting the final RMSE value to a scale which can be interpreted directly in relation to your raw data scales\n",
    "* As such, this model is the most popular of the 3 mentioned\n",
    "\n",
    "### R-Squared (r2)\n",
    "* This is not technically an error metric like the above\n",
    "* a.k.a. **co-efficient of regression**\n",
    "* It's essentially a measure of how much variance your model accounts for\n",
    "* It ranges between 0 and 1 (where a value of 0.9 means your model describes 90% of the variance of the data)\n",
    "* In general the higher the better\n",
    "* There are also variants, such as adjusted r2 value\n",
    "    * r2 simply tells you how much of the variance in your outputs is described by your inputs\n",
    "    * The higher the r2, the better your model is at interpreting the relationship between inputs and outputs\n",
    "    * However, if you add more variables to your model, the r2 squared value will increase regardless of whether or not the new variables are significantly impacting variance in your outputs\n",
    "    * As such, adjusted r2 will penalize your model (i.e. return a lower value) if new variables are added which do not improve your model or significantly affect your outputs\n",
    "    * Therefore, for multiple regression models, adjust r2 should always be used to avoid the raw r2 value being 'gamed' by simply adding more variables\n",
    "    \n",
    "## 3.) Real World Linear Regression\n",
    "### Load in Data\n",
    "* Most real world data won't be in a neat, Spark compatible format already\n",
    "* i.e. it won't have 1 column of labels and 1 column of all features\n",
    "* As such, we will explore how to transform data into a Spark-friendly format below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "excellent-turtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "df = spark.read.csv('Data/Ecommerce_Customers.csv', inferSchema=True, header=True)\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "enclosed-buddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mstephenson@fernandez.com\n",
      "835 Frank TunnelWrightmouth, MI 82180-9605\n",
      "Violet\n",
      "34.49726772511229\n",
      "12.65565114916675\n",
      "39.57766801952616\n",
      "4.0826206329529615\n",
      "587.9510539684005\n"
     ]
    }
   ],
   "source": [
    "# peek at data\n",
    "for item in df.head(1)[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-rating",
   "metadata": {},
   "source": [
    "### Data as Vectors\n",
    "* As we saw above, Spark ML requires data to have features and labels in 1 column each\n",
    "* Below we use the VectorAssembler to convert our range of features into a single features column\n",
    "* We can then perform standard processes on this data (i.e. train/test split, model fitting etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cardiac-cheat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Email='mstephenson@fernandez.com', Address='835 Frank TunnelWrightmouth, MI 82180-9605', Avatar='Violet', Avg Session Length=34.49726772511229, Time on App=12.65565114916675, Time on Website=39.57766801952616, Length of Membership=4.0826206329529615, Yearly Amount Spent=587.9510539684005, features=DenseVector([34.4973, 12.6557, 39.5777, 4.0826]))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries to process data into vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create an assembler which converts features into single feature column/vector\n",
    "# this tells the assembler what to expect, it doesn't actually create the output\n",
    "assembler = VectorAssembler(inputCols=['Avg Session Length', 'Time on App',\n",
    "                                       'Time on Website', 'Length of Membership'],\n",
    "                            outputCol='features')\n",
    "\n",
    "# create single output vector from multiple input features\n",
    "output = assembler.transform(df)\n",
    "\n",
    "# show output\n",
    "# see 'features=[a, b, c, d]' where a single vector\n",
    "# contains one value per original input feature\n",
    "output.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "weighted-alfred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            features|Yearly Amount Spent|\n",
      "+--------------------+-------------------+\n",
      "|[34.4972677251122...|  587.9510539684005|\n",
      "|[31.9262720263601...|  392.2049334443264|\n",
      "|[33.0009147556426...| 487.54750486747207|\n",
      "+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select final data after above process\n",
    "# simply features and labels in 2 columns\n",
    "final = output.select('features', 'Yearly Amount Spent')\n",
    "\n",
    "# show final data\n",
    "final.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "featured-celtic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|0.07758416715586236|\n",
      "| 10.170815702707046|\n",
      "| 6.1357435255481505|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split train and test data\n",
    "train, test = final.randomSplit([0.7, 0.3])\n",
    "\n",
    "# create linear regression model\n",
    "lr = LinearRegression(featuresCol='features',\n",
    "                      labelCol='Yearly Amount Spent',\n",
    "                      predictionCol='predictions')\n",
    "\n",
    "# fit model to training data\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# evaluate test results\n",
    "# i.e. how good is our trained model on our test data\n",
    "test_results = lr_model.evaluate(test)\n",
    "\n",
    "# check residuals\n",
    "# i.e. actual - test variance\n",
    "test_results.residuals.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-parent",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "* This model is a pretty good one, our r2 value is 98.6% which is very strong\n",
    "* Our RMSE is also quite low compared to the actual units of our data (i.e. RMSE = 9.5 compared to our data mean of 499.3)\n",
    "* It's always worth comparing scoring metrics to your actual values where possible (although obviously if you're using e.g. MSE then the scale of the metric isn't directly comparable)\n",
    "* Note that the score here is very high and often for simple models (i.e. linear regression) it's unlikely your model will ever be this good. If you encounter this in the real world, always double check your results, it's only because this dataset is a model that the algorithm fits so well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "collaborative-passion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.460425615133678"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show RMSE\n",
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "biological-display",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855444227766031"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show r2\n",
    "test_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "comic-current",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                500|\n",
      "|   mean|  499.3140382585909|\n",
      "| stddev|   79.3147815497068|\n",
      "|    min| 256.67058229005585|\n",
      "|    max|  765.5184619388373|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare to original data\n",
    "final.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "exposed-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|       predictions|\n",
      "+--------------------+------------------+\n",
      "|[30.5743636841713...| 441.9868295909098|\n",
      "|[30.7377203726281...|451.60992649352283|\n",
      "|[30.9716756438877...|488.50286623134457|\n",
      "+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract features only from test to create unlabelled data\n",
    "unlabelled_data = test.select('features')\n",
    "\n",
    "# make predictions of labels using test features\n",
    "predictions = lr_model.transform(unlabelled_data)\n",
    "\n",
    "# check outputs\n",
    "predictions.show(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
