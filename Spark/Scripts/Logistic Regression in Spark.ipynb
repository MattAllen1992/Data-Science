{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "usual-april",
   "metadata": {},
   "source": [
    "# Logistic Regression in Spark\n",
    "## 1.) Basic Logistic Regression\n",
    "* Logistic regression models are classification algorithms\n",
    "* They use a logistic function to map input values to an output ranging between 0 and 1\n",
    "* These output values are the probability that the given inputs result in a 0 or a 1\n",
    "* As such, this is a binary classification method\n",
    "* Below, we build a simple logistic regression model and fit it to our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "south-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find spark\n",
    "import findspark\n",
    "\n",
    "# point to spark\n",
    "findspark.init('/home/matt/spark-3.0.2-bin-hadoop3.2')\n",
    "\n",
    "# load spark lib\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create session\n",
    "spark = SparkSession.builder.appName('logreg').getOrCreate()\n",
    "\n",
    "# load logreg libs\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# read in data\n",
    "df = spark.read.format('libsvm').load('Data/sample_libsvm_data.txt')\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "representative-tract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peek at data\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bored-firmware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create logreg model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# fit model to data\n",
    "lr_model = lr.fit(df)\n",
    "\n",
    "# show model summary\n",
    "lr_summary = lr_model.summary\n",
    "\n",
    "# inspect summary\n",
    "# the predictions object is a df\n",
    "# we can extract label, feature, predictions and probs from this\n",
    "lr_summary.predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "apart-rapid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we want to check if the actual 'label' matches the predicted results\n",
    "# at a glance this looks like a pretty good estimator so far\n",
    "lr_summary.predictions.select('label', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-average",
   "metadata": {},
   "source": [
    "## 2.) Evaluating the Model\n",
    "* The above code offers simplistic evaluation of our model\n",
    "* We can extract basic info such as the probability and predicted binary class of each input\n",
    "* However, we can use Spark **evaluators** to go into more detail in this area\n",
    "* This includes things like confusion matrices, classification reports, ROC curves etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "choice-republic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[122,123,124...|[17.3275120737099...|[0.99999997016286...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[30.0713136181052...|[0.99999999999991...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[30.8791077666983...|[0.99999999999996...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[22.2580906352772...|[0.99999999978450...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[22.2560261974221...|[0.99999999978406...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[17.2672598669120...|[0.99999996830984...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[30.1193826680919...|[0.99999999999991...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[23.5627343744599...|[0.99999999994154...|       0.0|\n",
      "|  0.0|(692,[129,130,131...|[16.1306561349988...|[0.99999990124820...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[24.4092982638156...|[0.99999999997492...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split train/test data\n",
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# create new model object\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# train model on train data\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# calculate predicted results\n",
    "test_pred = lr_model.evaluate(test)\n",
    "\n",
    "# show output df\n",
    "test_pred.predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "indonesian-measure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load evaluator libs\n",
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator,\n",
    "                                   MulticlassClassificationEvaluator)\n",
    "\n",
    "# create binary evaluator instance\n",
    "binary_eval = BinaryClassificationEvaluator()\n",
    "\n",
    "# evaluate predictions \n",
    "final_roc = binary_eval.evaluate(test_pred.predictions)\n",
    "\n",
    "# show results\n",
    "# this represents area under the curve\n",
    "# 1 means perfect, which is unlikely in reality\n",
    "# but this is model data hence the perfect accuracy\n",
    "final_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-stephen",
   "metadata": {},
   "source": [
    "# Titanic Classification\n",
    "## 1.) First Steps\n",
    "* The titanic data contains information about passengers and whether or not they survived\n",
    "* We will use this information to build a logistic regression classification model\n",
    "* This model will predict whether or not a passenger will survive based on their information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "regular-tucson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "df = spark.read.csv('Data/titanic.csv', inferSchema=True, header=True)\n",
    "\n",
    "# check schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ready-channels",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peek at data\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-earthquake",
   "metadata": {},
   "source": [
    "## 2.) Prepare Features\n",
    "* We need to perform categorical indexing and encoding so that we can handle our categorical variables\n",
    "* We also need to vectorize all of our features into a Spark friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "soviet-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant cols\n",
    "# drop ID and text cols\n",
    "df_clean = df.select(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])\n",
    "\n",
    "# drop nulls\n",
    "df_final = df_clean.na.drop()\n",
    "\n",
    "# load vector and encoder libs\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, OneHotEncoder, StringIndexer\n",
    "\n",
    "### INDEXERS and ENCODERS ###\n",
    "# create gender indexer\n",
    "# e.g. A, B, C > 0, 1, 2\n",
    "gender_indexer = StringIndexer(inputCol='Sex', outputCol='SexIndex')\n",
    "\n",
    "# create gender encoder\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex', outputCol='SexVec')\n",
    "\n",
    "# create embark indexer\n",
    "embark_indexer = StringIndexer(inputCol='Embarked', outputCol='EmbarkedIndex')\n",
    "\n",
    "# create embark encoder\n",
    "embark_encoder = OneHotEncoder(inputCol='EmbarkedIndex', outputCol='EmbarkedVec')\n",
    "\n",
    "### ASSEMBLER ###\n",
    "# assemble all input cols (incl. above encoded categories)\n",
    "assembler = VectorAssembler(inputCols=['Pclass', 'SexVec', 'EmbarkedVec', 'Age', 'SibSp', 'Parch', 'Fare'],\n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-reduction",
   "metadata": {},
   "source": [
    "## 3.) Build Model Pipeline\n",
    "* The above steps are defining what we plan on doing\n",
    "* But we have not executed any of these steps, we've simply laid out the steps to perform\n",
    "* As such, we now need to run our steps and we can combine these into a pipeline for ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "nervous-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libs\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# create logreg instance\n",
    "titanic_logreg = LogisticRegression(featuresCol='features',\n",
    "                                    labelCol='Survived')\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline(stages=[gender_indexer, embark_indexer,\n",
    "                            gender_encoder, embark_encoder,\n",
    "                            assembler, titanic_logreg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-illinois",
   "metadata": {},
   "source": [
    "## 4.) Build and Evaluate Model\n",
    "* Now we've done that, we can build and train our model\n",
    "* Then we can evaluate the results of our model\n",
    "* We will simply look at the AUC of our model, i.e. % accuracy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "starting-tablet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split train/test data\n",
    "train, test = df_final.randomSplit([0.7, 0.3])\n",
    "\n",
    "# fit model to training data\n",
    "fit_model = pipeline.fit(train)\n",
    "\n",
    "# make predictions on test data\n",
    "# this creates 'prediction' column storing predictions\n",
    "# hence why we pass this as a string in the later eval step\n",
    "results = fit_model.transform(test)\n",
    "\n",
    "# load evaluation libs\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# build evaluator object\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Survived')\n",
    "\n",
    "# show results\n",
    "results.select('Survived', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "beautiful-terrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7472972972972973"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "AUC = my_eval.evaluate(results)\n",
    "\n",
    "# show result\n",
    "AUC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
