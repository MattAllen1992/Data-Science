{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proved-patio",
   "metadata": {},
   "source": [
    "# PySpark DataFrames\n",
    "## 1.) Create a Session\n",
    "* Spark requires an active session in order to use its functionality\n",
    "* This is accessed via the below libraries/code\n",
    "* Note that we've also used **findspark** to allow us to use Spark from any directory, not just within the directory that the files are kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "casual-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load findspark\n",
    "import findspark\n",
    "\n",
    "# tell code where spark files are kept\n",
    "findspark.init('/home/matt/spark-3.0.2-bin-hadoop3.2/')\n",
    "\n",
    "# load libraries for spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create session\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-fortune",
   "metadata": {},
   "source": [
    "## 2.) Read Data\n",
    "* We can use Spark to directly read a range of input data formats\n",
    "* Here we are loading json but you can use **shift + tab** after the 'read.' method to see the other available data formats\n",
    "* **.show()** lets us look at the dataframe content\n",
    "* We can use **printSchema()** to show us the dataframe schema also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beautiful-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data into df\n",
    "df = spark.read.json('people.json')\n",
    "\n",
    "# peek at data\n",
    "# note that nulls aren't an issue\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "basic-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show df schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "american-remark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show column names as list\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indian-samuel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show statistical summary of df\n",
    "# note that it shows non-numerica variables also\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-clear",
   "metadata": {},
   "source": [
    "## 3.) Schemas\n",
    "* Dataframes in Spark require a schema to define the column names, types and ability to handle nulls\n",
    "* There are a number of other properties they can have too which we will come onto\n",
    "* Below, the code shows how to manually define a schema for a dataframe to ensure data types and missing data is handled correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "japanese-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data types for schema definition\n",
    "from pyspark.sql.types import (StructField, StringType,\n",
    "                               IntegerType, StructType)\n",
    "\n",
    "# create list of structure fields\n",
    "# create a dataframe column schema called age which stores integers and True allows nulls\n",
    "data_schema = [StructField('age', IntegerType(), True),\n",
    "               StructField('name', StringType(), True)]\n",
    "\n",
    "# create dataframe schema based on above column definitions\n",
    "final_struct = StructType(fields=data_schema)\n",
    "\n",
    "# load data into structure\n",
    "df = spark.read.json('people.json', schema=final_struct)\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-longer",
   "metadata": {},
   "source": [
    "## 4.) Manipulating DataFrames\n",
    "* As in pandas, Spark dataframes offer you a wide range of functionality\n",
    "* Below are a few basic examples, we will look at more complex ones later one\n",
    "* Simple column and row extraction, creation of new columns, renaming, selection of specific data etc. are covered below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "balanced-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# indexing will return the entire column as a single object\n",
    "df['age']\n",
    "\n",
    "# select specific column using Spark select\n",
    "df.select('age')\n",
    "\n",
    "# this actually returns a dataframe of your column data\n",
    "# this is more useful when wanting to actually do something with the data\n",
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "white-kitty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=None, name='Michael')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show top n rows\n",
    "df.head(2)\n",
    "\n",
    "# extract specific row\n",
    "df.head(2)[0] # get first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "universal-employee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select multiple columns and show all selected data\n",
    "df.select(['age', 'name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "introductory-dietary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|double_age|\n",
      "+----+-------+----------+\n",
      "|null|Michael|      null|\n",
      "|  30|   Andy|        60|\n",
      "|  19| Justin|        38|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create new column\n",
    "# duplicate age column and double\n",
    "df.withColumn('double_age', df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fancy-honduras",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note that the above creation of columns does not\n",
    "# overwrite data in the original dataframe (see below)\n",
    "# to overwrite, you would have to store the above code\n",
    "# in a new variable and create a column in place\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "municipal-chart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|renamed_age|   name|\n",
      "+-----------+-------+\n",
      "|       null|Michael|\n",
      "|         30|   Andy|\n",
      "|         19| Justin|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename column\n",
    "df.withColumnRenamed('age', 'renamed_age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-pottery",
   "metadata": {},
   "source": [
    "## 5.) SQL with Dataframes\n",
    "* You can use SQL queries to interact with and extract specific data from your dataframes\n",
    "* The Spark library we imported above is called SQL because of its ability to utilise SQL language queries with Spark dataframes\n",
    "* The above code is python PySpark code to interact with dataframes, whilst the below shows you the SQL equivalent\n",
    "* You can choose which you use, or use a combination of the two\n",
    "* The beauty of this is absolute flexibility and cross-platform creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "electrical-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register df as temporary SQL view\n",
    "df.createOrReplaceTempView('people')\n",
    "\n",
    "# directly query df with SQL\n",
    "results = spark.sql('SELECT * FROM people WHERE age=30')\n",
    "\n",
    "# show results\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-spyware",
   "metadata": {},
   "source": [
    "## 6.) DataFrame Operations\n",
    "* The main purpose of storing data in Spark dataframes is so that you can extract exactly what you're after\n",
    "* Below are a few of the more common operations such as filtering rows with conditional logic, selecting specific columns and so on\n",
    "* Note that you can either use SQL based syntax or pythonic syntax depending on preference\n",
    "* If you want to simply view filtered data then a combination of filter, select etc. followed by show() will do this\n",
    "* If you actually want to store your filtered data then **collect()** should be used in conjunction with variable assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deluxe-threat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data into df\n",
    "# guess schema automatically and read first row as header\n",
    "df = spark.read.csv('appl_stock.csv', inferSchema=True, header=True)\n",
    "\n",
    "# peek at df schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "assumed-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peek at data\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "registered-astronomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      Open|               Low|\n",
      "+----------+------------------+\n",
      "|213.429998|212.38000099999996|\n",
      "|214.599998|        213.249994|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use SQL based code to filter df\n",
    "df.filter(\"Close < 500\").select(['Open', 'Low']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "failing-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      Open|               Low|\n",
      "+----------+------------------+\n",
      "|213.429998|212.38000099999996|\n",
      "|214.599998|        213.249994|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is the more pythonic way of performing the same operation as above\n",
    "df.filter(df['Close'] < 500).select(df['Open'], df['Low']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "expected-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|      Date|              Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+----------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|2010-02-01|192.36999699999998|     196.0|191.29999899999999|194.729998|187469100|         25.229131|\n",
      "|2010-02-02|        195.909998|196.319994|193.37999299999998|195.859997|174585600|25.375532999999997|\n",
      "+----------+------------------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conditional operators require separation via brackets\n",
    "# and also &, | or ~ symbols (and, or and not won't work)\n",
    "df.filter((df['Close'] < 200) & ~(df['Open'] > 200)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "applied-ridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': '2010-01-22',\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find exact values and collect into var\n",
    "# returns a list of all matching row objects\n",
    "result = df.filter(df['Low'] == 197.16).collect()\n",
    "\n",
    "# extract first row from result\n",
    "row = result[0]\n",
    "\n",
    "# store row data in dictionary\n",
    "row_dict = row.asDict()\n",
    "\n",
    "# show dict\n",
    "row_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "packed-indie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220441900"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract specific value\n",
    "row_dict['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-punch",
   "metadata": {},
   "source": [
    "## 7.) Group By and Aggregate\n",
    "* In order to summarize your data you can use group by and aggregate functions\n",
    "* Group by summarizes your data by specific fields (i.e. group by company name)\n",
    "* Aggregate functions allow you to summarize your data in specific ways (e.g. count, sum, max, min, avg...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "asian-lebanon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data into df\n",
    "df = spark.read.csv('sales_info.csv', inferSchema=True, header=True)\n",
    "\n",
    "# check schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "elder-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peek at data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "steady-celtic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by specific criteria\n",
    "#df.groupBy('Company').mean().show()\n",
    "#df.groupBy('Company').max().show()\n",
    "#df.groupBy('Company').min().show()\n",
    "#df.groupBy('Company').count().show()\n",
    "df.groupBy('Company').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "equal-currency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate across all rows\n",
    "#df.agg({'Sales':'max'}).show()\n",
    "df.agg({'Sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "original-employment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter using group by\n",
    "group_data = df.groupBy('Company')\n",
    "\n",
    "# show aggregate function of filtered data\n",
    "group_data.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-neighborhood",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* As well as the above, in-built functions, you can also import more advanced functions from Spark\n",
    "* There are many libraries and methods to be used, below shows a few samples\n",
    "* These functions are commonly used within a select call\n",
    "* You can pass an alias to define the name of your output column/data\n",
    "* Formatting can be applied to tidy up your outputs using the **format_number** library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fifteen-macedonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Distinct Sales|\n",
      "+--------------+\n",
      "|            11|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load specific aggregate functions\n",
    "from pyspark.sql.functions import countDistinct, avg, stddev\n",
    "\n",
    "# apply function to specific df column\n",
    "df.select(countDistinct('Sales').alias('Distinct Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "earned-sellers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        Avg Sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to specific df column\n",
    "df.select(avg('Sales').alias('Avg Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "august-fluid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|STD Sales|\n",
      "+---------+\n",
      "|   250.09|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# libraries to format output data\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "# apply function to specific df column\n",
    "sales_std = df.select(stddev('Sales').alias('STD Sales'))\n",
    "\n",
    "# show output data with 2 d.p.\n",
    "sales_std.select(format_number('STD Sales', 2).alias('STD Sales')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-warren",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* You can also order and sort your data using the below code\n",
    "* You are able to specify asc/desc order as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "consolidated-guide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# order by specific column, asc by default\n",
    "df.orderBy('Sales').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "second-consciousness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|Company|Person|Sales|\n",
      "+-------+------+-----+\n",
      "|     FB|  Carl|870.0|\n",
      "|   APPL|  Mike|750.0|\n",
      "|   MSFT|  Tina|600.0|\n",
      "+-------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# order by in descending order (requires pythonic format)\n",
    "df.orderBy(df['Sales'].desc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-thought",
   "metadata": {},
   "source": [
    "## 8.) Missing Data\n",
    "* Often, data contains missing values in specific areas\n",
    "* Many processes will break when trying to compute missing data, so it normally has to be handled\n",
    "* 3 basic options:\n",
    "    * Keep missing data\n",
    "    * Drop missing data\n",
    "    * Impute replacement data (based on e.g. mean, mode etc.)\n",
    "    \n",
    "### a.) Drop Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "recent-custom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data into df\n",
    "df = spark.read.csv('ContainsNull.csv', inferSchema=True, header=True)\n",
    "\n",
    "# check schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "familiar-prescription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peek at data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "trying-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop all null rows\n",
    "# use shift + tab after na. to show options\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "intense-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop all nulls if there are\n",
    "# at least 2 nulls within the row\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "editorial-medium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop rows if all row values are null\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "confident-collectible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop rows if sales are null\n",
    "df.na.drop(subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-tragedy",
   "metadata": {},
   "source": [
    "### b.) Fill Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "comfortable-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|  Id|      Name|Sales|\n",
      "+----+----------+-----+\n",
      "|emp1|      John| null|\n",
      "|emp2|FILL VALUE| null|\n",
      "|emp3|FILL VALUE|345.0|\n",
      "|emp4|     Cindy|456.0|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill string columns only with specified value\n",
    "# understands it should fill a string column only\n",
    "df.na.fill('FILL VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "posted-indiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill numeric columns only with specified value\n",
    "# understands it should fill a numeric column only\n",
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "original-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explicitly fill selected column only\n",
    "df.na.fill('No Name', subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "variable-holmes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load libraries to impute missing data\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# get mean of column\n",
    "mean_val = df.select(mean(df['Sales'])).collect()\n",
    "\n",
    "# get mean value itself\n",
    "# need to be double indexed due to nature of value itself\n",
    "mean_sales = mean_val[0][0]\n",
    "\n",
    "# fill null numeric column values with mean of column\n",
    "df.na.fill(mean_sales, subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "recent-trauma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do the exact same as above except in one line\n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0], subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-individual",
   "metadata": {},
   "source": [
    "## 9.) Dates and Timestamps\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "varying-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data into df\n",
    "df = spark.read.csv('appl_stock.csv', inferSchema=True, header=True)\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "developed-regression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peek at data\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eight-dallas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load libraries for handling datetime\n",
    "from pyspark.sql.functions import (dayofmonth, hour, dayofyear,\n",
    "                                   month, year, weekofyear,\n",
    "                                   format_number, date_format)\n",
    "\n",
    "# apply function to specific df column\n",
    "df.select(dayofmonth(df['Date'])).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "educated-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|hour(Date)|\n",
      "+----------+\n",
      "|         0|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show hour at which time was recorded\n",
    "df.select(hour(df['Date'])).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "proved-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|month(Date)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the month recorded\n",
    "df.select(month(df['Date'])).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-michigan",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* The above code is fairly simplistic, often you'll need to aggregate data in combination with handling timestamp formats etc.\n",
    "* Below we show a potential use case where we are grouping by an extracted date component before aggregating a numeric column to show average closing price by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cutting-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|Avg Close|\n",
      "+----+---------+\n",
      "|2010|   259.84|\n",
      "|2011|   364.00|\n",
      "|2012|   576.05|\n",
      "|2013|   472.63|\n",
      "|2014|   295.40|\n",
      "|2015|   120.04|\n",
      "|2016|   104.60|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create new column in new df containing year (extracted from date)\n",
    "new_df = df.withColumn('Year', year(df['Date']))\n",
    "\n",
    "# get average closing price by year and sort by year\n",
    "result = new_df.groupBy('Year').mean().select(['Year', 'avg(Close)']).orderBy('Year')\n",
    "\n",
    "# rename column\n",
    "new = result.withColumnRenamed('avg(Close)', 'Average Closing Price')\n",
    "\n",
    "# format values\n",
    "new.select(['Year', format_number('Average Closing Price', 2).alias('Avg Close')]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
