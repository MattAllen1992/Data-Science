{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lonely-johns",
   "metadata": {},
   "source": [
    "# Decision Trees in Spark\n",
    "## 1.) Basic Overview\n",
    "* Decision trees can be used for regression and classification tasks\n",
    "* They sequentially segment your data into smaller and smaller subsets based on specific feature values/thresholds in order to produce the highest class purity (for classification) or closest mean value (for regression) outputs\n",
    "* The aim is to reduce error or misclassification at each step of the process in a greedy manner (i.e. focus on current step specifically rather than looking forwards to future steps)\n",
    "* Pruning is used to reduce tree depth (and hence number of splits) once the full tree has been built in order to reduce overfitting, which occurs strongly in decision tree learning otherwise\n",
    "* Decision trees are easy to interpret and visualize, following human logic in terms of simple splits based on clear criteria\n",
    "* However, they often don't perform as well as other methods (e.g. linear regression) for comparable problems and as such, enhancements on simple trees such as random forests, bagging and boosting are required\n",
    "    * Bagging involves bootstrapping (splitting input data into multiple subsets to allow ~cross-validation) and then aggregation (running 100s or 1000s of independent trees on the data subsets before averaging results/residiuals/learnings across all trees to create a balanced model)\n",
    "    * Random Forests are similar to bagging except that the features allowed in each tree are restricted at random to ensure that each predictor is focused on in different trees to prevent only focusing on the most important features in regards to the outcomes\n",
    "    * Boosting involves sequentially enhancing the model using shallow trees (often stumps with a depth of 1) and applying a shrinkage parameter to create a slow, gradual learning method to prevent overfitting\n",
    "* Across all methods and variants, the aim is to reduce the error of the model whilst preventing overfitting\n",
    "    * For regression, residual sum of squares (RSS) is used to determine error between actual values at terminal nodes in comparison with the mean value of results that end up at each terminal node\n",
    "    * For classification, classification error, Gini score or cross-entropy scores are used to assess class purity at each node as well as comparing predicted classes to actual classes with the aim of achieving the best class purity possible at terminal nodes\n",
    "* NOTE: you can also get decision tree based models via the pyspark.ml.regression library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "thick-engine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load libs\n",
    "import findspark\n",
    "\n",
    "# store location of spark files\n",
    "findspark.init('/home/matt/spark-3.0.2-bin-hadoop3.2')\n",
    "\n",
    "# load libs\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start new session\n",
    "spark = SparkSession.builder.appName('tree').getOrCreate()\n",
    "\n",
    "# load other libs\n",
    "# note that these are classifier models, regression models exist too\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\n",
    "\n",
    "# read in data\n",
    "df = spark.read.format('libsvm').load('Data/sample_libsvm_data.txt')\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "undefined-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peek at data\n",
    "# already formatted for Spark\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cultural-acceptance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------------+-----------+----------+\n",
      "|label|            features|rawPrediction|probability|prediction|\n",
      "+-----+--------------------+-------------+-----------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|   [33.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[122,123,148...|   [33.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|   [33.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "+-----+--------------------+-------------+-----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split train/test data\n",
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# build classifier models\n",
    "# can change maxDepth etc. here\n",
    "# testing tree amounts and accuracy is useful for seeing\n",
    "# where you stop gaining benefits by increasing tree num\n",
    "dtc = DecisionTreeClassifier()\n",
    "rfc = RandomForestClassifier(numTrees=100)\n",
    "gbtc = GBTClassifier()\n",
    "\n",
    "# fit models to training data\n",
    "dtc_model = dtc.fit(train)\n",
    "rfc_model = rfc.fit(train)\n",
    "gbtc_model = gbtc.fit(train)\n",
    "\n",
    "# make predictions\n",
    "dtc_preds = dtc_model.transform(test)\n",
    "rfc_preds = rfc_model.transform(test)\n",
    "gbtc_preds = gbtc_model.transform(test)\n",
    "\n",
    "# check outputs\n",
    "dtc_preds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "least-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC Accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import evaluators\n",
    "# binary evaluators only give you AUC, ROC etc.\n",
    "# multiclass still works on binary data but also lets you pull more metrics (see below)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# create evaluator\n",
    "acc_eval = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "\n",
    "# evaluate decision tree\n",
    "# this accuracy is perfect which is worrying\n",
    "# but our data is model and so highly separable\n",
    "# also, basic decision trees are prone to overfitting\n",
    "# this analysis is a simple example and not a real world scenario\n",
    "print('DTC Accuracy: ')\n",
    "acc_eval.evaluate(dtc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ruled-johnston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(692, {149: 0.001, 154: 0.0003, 155: 0.0016, 177: 0.0007, 178: 0.001, 179: 0.0023, 183: 0.0005, 210: 0.0025, 215: 0.0009, 234: 0.0028, 236: 0.0059, 237: 0.0005, 241: 0.0015, 244: 0.0057, 260: 0.0006, 262: 0.0076, 263: 0.0071, 266: 0.0004, 270: 0.0004, 271: 0.0026, 272: 0.0073, 273: 0.0075, 288: 0.0003, 295: 0.001, 297: 0.0007, 300: 0.0074, 302: 0.0063, 317: 0.0153, 319: 0.0004, 323: 0.0006, 325: 0.0009, 326: 0.0004, 330: 0.0129, 332: 0.001, 344: 0.006, 345: 0.0075, 346: 0.0046, 347: 0.001, 350: 0.0183, 351: 0.0254, 352: 0.0011, 355: 0.001, 356: 0.0143, 357: 0.0121, 358: 0.0005, 360: 0.0012, 369: 0.0005, 372: 0.011, 373: 0.0099, 374: 0.0167, 375: 0.0061, 377: 0.0001, 378: 0.0285, 379: 0.0161, 381: 0.0005, 383: 0.0005, 385: 0.0214, 386: 0.0086, 388: 0.0022, 398: 0.0006, 401: 0.0005, 403: 0.0022, 404: 0.0004, 405: 0.01, 406: 0.0226, 407: 0.0393, 409: 0.001, 410: 0.0017, 411: 0.0033, 413: 0.0229, 415: 0.0003, 425: 0.0027, 426: 0.0054, 427: 0.0056, 429: 0.0224, 433: 0.0209, 434: 0.059, 435: 0.025, 442: 0.006, 453: 0.002, 454: 0.0027, 455: 0.0407, 456: 0.0004, 457: 0.0088, 460: 0.0009, 461: 0.0406, 462: 0.0342, 463: 0.0012, 468: 0.0235, 469: 0.0005, 471: 0.0048, 472: 0.0005, 481: 0.0083, 482: 0.0015, 484: 0.0265, 485: 0.0016, 488: 0.0048, 489: 0.0196, 490: 0.0054, 491: 0.0005, 492: 0.0004, 493: 0.0015, 495: 0.0014, 496: 0.0069, 497: 0.0169, 500: 0.0007, 510: 0.0019, 511: 0.0268, 512: 0.0196, 516: 0.0017, 517: 0.0475, 518: 0.0061, 519: 0.004, 522: 0.0009, 524: 0.0062, 525: 0.0015, 539: 0.0041, 540: 0.0181, 545: 0.0096, 547: 0.0007, 565: 0.0005, 567: 0.0011, 568: 0.0081, 569: 0.0001, 581: 0.0016, 599: 0.0002, 600: 0.0001, 601: 0.0007, 603: 0.0009, 604: 0.0005, 626: 0.0002, 627: 0.0009, 628: 0.0012, 633: 0.0004, 665: 0.0005, 678: 0.0005, 689: 0.0019})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get feature importances from model\n",
    "# trees determine which features are most significant\n",
    "# in relation to the outcome labels\n",
    "# shows importance by feature\n",
    "rfc_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-barbados",
   "metadata": {},
   "source": [
    "## 2. Real World Data\n",
    "### Building the Model\n",
    "* Here, we will process more realistic data by converting features into a Spark-friendly format\n",
    "* We will also use a pipeline to chain together the multiple stages involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "planned-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- School: string (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: integer (nullable = true)\n",
      " |-- Accept: integer (nullable = true)\n",
      " |-- Enroll: integer (nullable = true)\n",
      " |-- Top10perc: integer (nullable = true)\n",
      " |-- Top25perc: integer (nullable = true)\n",
      " |-- F_Undergrad: integer (nullable = true)\n",
      " |-- P_Undergrad: integer (nullable = true)\n",
      " |-- Outstate: integer (nullable = true)\n",
      " |-- Room_Board: integer (nullable = true)\n",
      " |-- Books: integer (nullable = true)\n",
      " |-- Personal: integer (nullable = true)\n",
      " |-- PhD: integer (nullable = true)\n",
      " |-- Terminal: integer (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- perc_alumni: integer (nullable = true)\n",
      " |-- Expend: integer (nullable = true)\n",
      " |-- Grad_Rate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = spark.read.csv('Data/College.csv', inferSchema=True, header=True)\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sunset-friendship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(School='Abilene Christian University', Private='Yes', Apps=1660, Accept=1232, Enroll=721, Top10perc=23, Top25perc=52, F_Undergrad=2885, P_Undergrad=537, Outstate=7440, Room_Board=3300, Books=450, Personal=2200, PhD=70, Terminal=78, S_F_Ratio=18.1, perc_alumni=12, Expend=7041, Grad_Rate=60)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek at data\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "equal-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libs\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "# create assembler\n",
    "assembler = VectorAssembler(inputCols=['Apps', 'Accept', 'Enroll', 'Top10perc', 'Top25perc', 'F_Undergrad',\n",
    "                                       'P_Undergrad', 'Outstate', 'Room_Board', 'Books', 'Personal',\n",
    "                                       'PhD', 'Terminal', 'S_F_Ratio', 'perc_alumni', 'Expend', 'Grad_Rate'],\n",
    "                            outputCol='features')\n",
    "\n",
    "# apply assembler to format features\n",
    "output = assembler.transform(df)\n",
    "\n",
    "# encode categorical vars (private = Yes/No)\n",
    "private_indexer = StringIndexer(inputCol='Private', outputCol='PrivateIndex')\n",
    "output_enc = private_indexer.fit(output).transform(output)\n",
    "\n",
    "# extract relevant vars\n",
    "final_df = output_enc.select('features', 'PrivateIndex')\n",
    "\n",
    "# train/test split\n",
    "train, test = final_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-authority",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "* We can see here that our single decision tree is a pretty good predictor of our data, a score of >90% with a single tree indicates that our data is quite easily separable\n",
    "* Clearly the random forest and gradient boosted trees outperform the decision tree though\n",
    "* This is what you would expect every time as, by definition, these are both enhanced, ensemble versions of the simple decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "authentic-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC:\n",
      "0.8907056798623063\n"
     ]
    }
   ],
   "source": [
    "# load libs\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# build model instances\n",
    "dtc = DecisionTreeClassifier(labelCol='PrivateIndex', featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='PrivateIndex', featuresCol='features', numTrees=150)\n",
    "gbt = GBTClassifier(labelCol='PrivateIndex', featuresCol='features')\n",
    "\n",
    "# fit models to training data\n",
    "dtc_model = dtc.fit(train)\n",
    "rfc_model = rfc.fit(train)\n",
    "gbt_model = gbt.fit(train)\n",
    "\n",
    "# make predictions\n",
    "dtc_preds = dtc_model.transform(test)\n",
    "rfc_preds = rfc_model.transform(test)\n",
    "gbt_preds = gbt_model.transform(test)\n",
    "\n",
    "# create evaluation metrics\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol='PrivateIndex')\n",
    "\n",
    "# show results per model\n",
    "print('DTC:')\n",
    "print(binary_eval.evaluate(dtc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "complimentary-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC:\n",
      "0.9842226047045317\n"
     ]
    }
   ],
   "source": [
    "# show results per model\n",
    "print('RFC:')\n",
    "print(binary_eval.evaluate(rfc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "final-witch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT:\n",
      "0.9731306177089308\n"
     ]
    }
   ],
   "source": [
    "# show results per model\n",
    "print('GBT:')\n",
    "print(binary_eval.evaluate(gbt_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "indie-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475982532751092"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the binary evaluator above doesn't give us many metrics\n",
    "# the multiclass evaluator lets us look at accuracy, precision, recall etc.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# create evaluator for accuracy\n",
    "# you can check other metrics by tweaking the metricName\n",
    "# check documentation for available options\n",
    "acc_eval = MulticlassClassificationEvaluator(labelCol='PrivateIndex',\n",
    "                                             metricName='accuracy')\n",
    "\n",
    "# evaluate rfc model\n",
    "rfc_acc = acc_eval.evaluate(rfc_preds)\n",
    "rfc_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
