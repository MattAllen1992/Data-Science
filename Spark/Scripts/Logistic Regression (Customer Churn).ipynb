{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chemical-relation",
   "metadata": {},
   "source": [
    "# Logistic Regression (Customer Churn)\n",
    "## 1.) Brief\n",
    "* Classification model to predict which customers are most likely to churn\n",
    "* The business can then decide which account managers to assign to high risk clients\n",
    "* The model will be trained on historical data and then be used on future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "native-orbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find spark\n",
    "import findspark\n",
    "\n",
    "# point to spark\n",
    "findspark.init('/home/matt/spark-3.0.2-bin-hadoop3.2')\n",
    "\n",
    "# load spark lib\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create session\n",
    "spark = SparkSession.builder.appName('churn').getOrCreate()\n",
    "\n",
    "# read in data\n",
    "df = spark.read.csv('Data/customer_churn.csv', inferSchema=True, header=True)\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rapid-abraham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Names='Cameron Williams', Age=42.0, Total_Purchase=11066.8, Account_Manager=0, Years=7.22, Num_Sites=8.0, Onboard_date='2013-08-30 07:00:40', Location='10265 Elizabeth Mission Barkerburgh, AK 89518', Company='Harvey LLC', Churn=1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek at data\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "french-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[25.0,9672.03,0.0...|  0.0|[4.87367702951321...|[0.99241280373953...|       0.0|\n",
      "|[26.0,8939.61,0.0...|  0.0|[6.46156069957429...|[0.99844008170767...|       0.0|\n",
      "|[27.0,8628.8,1.0,...|  0.0|[5.48329700525221...|[0.99586159156313...|       0.0|\n",
      "|[28.0,11128.95,1....|  0.0|[4.3068478816979,...|[0.98670322625939...|       0.0|\n",
      "|[28.0,11204.23,0....|  0.0|[2.02574404872598...|[0.88347365099455...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libs\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# drop irrelevant columns\n",
    "# drop name, onboard date, location and company\n",
    "# name and date not relevant to churn, account manager randomly assigned but keep just in case\n",
    "\n",
    "# assemble data into vectors\n",
    "assembler = VectorAssembler(inputCols=['Age', 'Total_Purchase', 'Account_Manager', 'Years', 'Num_Sites'],\n",
    "                            outputCol='features')\n",
    "\n",
    "# vectorize data\n",
    "output = assembler.transform(df)\n",
    "\n",
    "# extract final data\n",
    "df_final = output.select('features', 'churn')\n",
    "\n",
    "# split train/test data\n",
    "train, test = df_final.randomSplit([0.7, 0.3])\n",
    "\n",
    "# build logreg model instance\n",
    "logreg = LogisticRegression(labelCol='churn')\n",
    "\n",
    "# train model on train data\n",
    "logreg_fit = logreg.fit(train)\n",
    "\n",
    "# summarise predictions\n",
    "train_summary = logreg_fit.summary\n",
    "train_summary.predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "short-graham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7429384170076854"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make test predictions\n",
    "pred = logreg_fit.evaluate(test)\n",
    "\n",
    "# classifier libs\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# create evaluator\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                        labelCol='churn')\n",
    "\n",
    "# evaluate model\n",
    "AUC = my_eval.evaluate(pred.predictions)\n",
    "\n",
    "# check AUC\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "noted-thread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       1.0|\n",
      "|Barron-Robertson|       1.0|\n",
      "|   Sexton-Golden|       1.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       1.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create final model on entire dataset\n",
    "final_model = logreg.fit(df_final)\n",
    "\n",
    "# read in new customer data\n",
    "new_df = spark.read.csv('Data/new_customers.csv', inferSchema=True, header=True)\n",
    "\n",
    "# transform new data\n",
    "new_df_tf = assembler.transform(new_df)\n",
    "\n",
    "# make predictions on new customers\n",
    "new_pred = final_model.transform(new_df_tf)\n",
    "\n",
    "# show predictions\n",
    "new_pred.select('Company', 'prediction').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
