{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Data: Feature Selection\n",
    "## Notes\n",
    "Project Links:\n",
    "* https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "* https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n",
    "* https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b\n",
    "\n",
    "Feature selection is the process of extracting only the most useful and relevant features from your source dataset, the main advantages include:\n",
    "* **Improving accuracy** by only using the most correlated or impacting variables.\n",
    "* **Reducing overfitting** by removing redundant/irrelevant variables which would just add to the noise of the data and result in model fitting of values which weren't relevant.\n",
    "* **Improving the speed** of the process by simply reducing the number of features and therefore data that needs to be processed.\n",
    "\n",
    "There are many ways of conducting feature selection so we will run through a few of the key ones here on a diabetes dataset which contains a number of numerical variables (features) as well as a class variable (0 or 1 to signify not having or having diabetes respectively). As a result, this is a classification problem in this case, it should be noted that different statistical methods of selecting features should be used depending on whether or not your problem is classification or regression and also which type of data you're using (i.e. numerical, bianry, categorical etc.).\n",
    "\n",
    "## Types of Feature Selection Methods\n",
    "There are 3 main categories of feature selection methods:\n",
    "* Feature methods\n",
    "* Wrapper methods\n",
    "* Embedded methods\n",
    "\n",
    "**Feature methods** look at one feature variable at a time (either in isolation or in the context of the target variable) and determine a score for each variable before keeping the variables with the highest score. An example of this would be the **univariate feature selection** section below which commonly uses chi-squared tests or correlation coefficients (e.g. in linear regressions).\n",
    "\n",
    "**Wrapper methods** look at the combination of features, build a model and then assess the score of the model as a whole. It will try different combinations and eliminations of features to determine the best model accuracy score possible. The search can be **methodical** (e.g. best first search), **stochastic** (i.e. random, e.g. random hill-climbing) or **heuristic** (i.e. autonomous, e.g. recursive feature elimination).\n",
    "\n",
    "**Embedded methods** are built into the development stages of the model itself, they will learn the best combination of features whilst the model is being built and use weighting and penalising to adjust the model on the fly. Most commonly these include **regularization methods** such as lasso, elastic net and ridge regression to introduce specific constraints on the model.\n",
    "\n",
    "## When to Apply Feature Selection\n",
    "It is important to build and apply the feature selection models/methods right **before you run the model**. This means that you should not build your feature selection model on the raw dataset, then split your sets down into train, test etc. and apply your model because in this case your feature selection will be enhanced by having seen the entire dataset and will likely be **overtrained and bias** on all data points.\n",
    "\n",
    "Instead, you should split your datasets down into their relevant folds (e.g. test/train or folds within cross-validation steps) and then **build your model right before applying it**. This way your feature extraction is specific to the subset of data only and can be compared to the overall dataset without as much bias.\n",
    "\n",
    "## 1. Univariate Feature Selection\n",
    "This method simply selects the most relevant features from our dataset for use in the final model. It uses a selected test or threshold to determine which feature variables are the most effective predictors of our target variable and removes the others. There are different ways of doing this, such as setting a score threshold and removing all features which don't meet that score, but here we will use **SelectKBest** which selects a specified number of features **k** from our dataset using a specified statistical test to score each variable.\n",
    "\n",
    "There are a number of different statistical tests you can use, the choice depends on the type of problem (e.g. regression or classification) and the data you're running it on (e.g. numerical, binary...). In this case, we're looking at a numerical classification problem so we will use **ANOVA F-value** via the **f_classif()** function in SKLearn.\n",
    "\n",
    "**Further notes:** https://machinelearningmastery.com/an-introduction-to-feature-selection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of times pregnant</th>\n",
       "      <th>Plasma glucose concentration</th>\n",
       "      <th>Diastolic blood pressure (mm Hg)</th>\n",
       "      <th>Triceps skin fold thickness (mm)</th>\n",
       "      <th>2-hour serum insulin (mm U/ml)</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Diabetes pedigree function</th>\n",
       "      <th>Age (years)</th>\n",
       "      <th>Diabetes (Binary Y/N)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of times pregnant  Plasma glucose concentration  \\\n",
       "0                         6                           148   \n",
       "1                         1                            85   \n",
       "2                         8                           183   \n",
       "3                         1                            89   \n",
       "4                         0                           137   \n",
       "\n",
       "   Diastolic blood pressure (mm Hg)  Triceps skin fold thickness (mm)  \\\n",
       "0                                72                                35   \n",
       "1                                66                                29   \n",
       "2                                64                                 0   \n",
       "3                                66                                23   \n",
       "4                                40                                35   \n",
       "\n",
       "   2-hour serum insulin (mm U/ml)   BMI  Diabetes pedigree function  \\\n",
       "0                               0  33.6                       0.627   \n",
       "1                               0  26.6                       0.351   \n",
       "2                               0  23.3                       0.672   \n",
       "3                              94  28.1                       0.167   \n",
       "4                             168  43.1                       2.288   \n",
       "\n",
       "   Age (years)  Diabetes (Binary Y/N)  \n",
       "0           50                      1  \n",
       "1           31                      0  \n",
       "2           32                      1  \n",
       "3           21                      0  \n",
       "4           33                      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('Diabetes.csv')\n",
    "\n",
    "# peek at data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (768, 8)\n",
      "[ 39.67022739 213.16175218   3.2569504    4.30438091  13.28110753\n",
      "  71.7720721   23.8713002   46.14061124]\n",
      "[[  6.  148.   33.6  50. ]\n",
      " [  1.   85.   26.6  31. ]\n",
      " [  8.  183.   23.3  32. ]\n",
      " [  1.   89.   28.1  21. ]\n",
      " [  0.  137.   43.1  33. ]]\n",
      "Output shape:  (768, 4)\n"
     ]
    }
   ],
   "source": [
    "# extract X and y variables\n",
    "array = df.values\n",
    "X = array[:, :8]\n",
    "y = array[:, 8]\n",
    "\n",
    "# check shape before\n",
    "print('Input shape: ', X.shape)\n",
    "\n",
    "# define feature extraction (pick 4 usinf ANOVA F-value)\n",
    "test = SelectKBest(score_func=f_classif, k=4) # build object\n",
    "fit = test.fit(X, y) # score each variable in X vs. y\n",
    "\n",
    "# extract features\n",
    "print(fit.scores_) # show test scores per variable\n",
    "features = fit.transform(X) # extract 4 features\n",
    "print(features[:5,:]) # show selected features top 5 rows\n",
    "print('Output shape: ', features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our original input had 8 features whilst the output following feature extraction has just 4 as specified. The scores for each variable are shown in the first array above and the method selected the 4 highest scores from this and produces the output array 'features' above.\n",
    "\n",
    "For help deciding which statistical test to use, see this link: https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n",
    "\n",
    "## 2. Recursive Feature Elimination (RFE)\n",
    "This method works by recursively to elminiate a variable from the dataset, build the model again, test the score and then continue to do so until it finds the optimum model accuracy/score. It can cross-validate itself to find the best specific combination of variables within the dataset. It works by identifying a score for each variable (e.g. **coef_** for linear regression variables or **feature_importances_** for others) and removes the variable(s) with the lowest scores.\n",
    "\n",
    "The method below uses RFE with a logistic regression function to find the 3 optimal variables in our dataset, the function used doesn't matter too much as long as it's consistent *(further reading needed here)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# build model\n",
    "model = LogisticRegression(solver='lbfgs') # logistic regression model\n",
    "rfe = RFE(model, 3) # pick 3 best features using logistic regression scoring\n",
    "\n",
    "# fit model\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "# analyse output\n",
    "print(\"Num Features: %d\" % fit.n_features_) # show final # of features\n",
    "print(\"Selected Features: %s\" % fit.support_) # show T/F inclusion of features\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_) # show ranking of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive model eliminated 5 features as specified to leave us with the 3 it deemed optimal.\n",
    "\n",
    "## 3. Principal Component Analysis (PCA)\n",
    "PCA is technically a **dimensionality reduction** process and so I will cover it in more detail in it's own notebook, but for now here's a basic overview of the code and concept.\n",
    "\n",
    "PCA is designed to pick the optimal components (i.e. features) from your dataset, again given a threshold or final number to select by you. The core concept of PCA is that it simplifies and compresses your dataset into fewer variables and then uses these as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.88854663 0.06159078 0.02579012]\n",
      "[[-2.02176587e-03  9.78115765e-02  1.60930503e-02  6.07566861e-02\n",
      "   9.93110844e-01  1.40108085e-02  5.37167919e-04 -3.56474430e-03]\n",
      " [-2.26488861e-02 -9.72210040e-01 -1.41909330e-01  5.78614699e-02\n",
      "   9.46266913e-02 -4.69729766e-02 -8.16804621e-04 -1.40168181e-01]\n",
      " [-2.24649003e-02  1.43428710e-01 -9.22467192e-01 -3.07013055e-01\n",
      "   2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]]\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3) # create model object, extract 3 features\n",
    "fit = pca.fit(X, y) # fit and extract\n",
    "\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output data above clearly looks nothing like the original input after it has been compressed and 'extracted'. Again though, this isn't pure feature extraction, it's dimensionality reduction as the input variables have been transformed rather than simply selected.\n",
    "\n",
    "## 4. Feature Importance\n",
    "Finally, you can calculate an **importance** score for each variable and use this to extract the most valuable variables in your analysis. This is particularly useful in **bagged decision tree** models such as Random Forest or Extra Trees where you need to determine which features to weight and use most.\n",
    "\n",
    "Here, we can use the **ExtraTreesClassifier** to calculate the importance of each variable in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10620699 0.24336721 0.09617079 0.08446838 0.07060376 0.14199293\n",
      " 0.11640966 0.14078028]\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=10) # create model\n",
    "model.fit(X, y) # fit model\n",
    "print(model.feature_importances_) # show importance of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the score the higher the feature's importance, the above results suggest that the 2nd, 6th and 8th columns are the most important, although you may want to interrogate and define your own thresholds.\n",
    "\n",
    "**Documentation:** https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
