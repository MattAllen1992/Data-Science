{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artistic-working",
   "metadata": {},
   "source": [
    "# Natural Language Processing Basics\n",
    "## 1. Tokenization\n",
    "### Tokenization:\n",
    "* The spacy library lets you read text\n",
    "* NLP objects perform a variety of actions on the text by default\n",
    "* Tokenization is the first, where it splits all text into tokens (i.e. constituent parts)\n",
    "* You can see that certain tokens are kept together rather than fully split out (e.g. U.S.)\n",
    "* This is due to the library that has been loaded\n",
    "* **Documents** are chunks of text which are to be parsed\n",
    "* **Modules** (e.g. nlp below) are the objects which let you parse text using their knowledge from the library you've loaded\n",
    "* **Spans** are sub-sections of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tribal-calculation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.S. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import spacy\n",
    "\n",
    "# load language library (small ENG) into spacy module (i.e. nlp)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# process text using nlp object\n",
    "doc = nlp(u\"Tesla isn\\'t looking at buying U.S. startup for $6 million\")\n",
    "\n",
    "# iterate through tokens from text\n",
    "for token in doc:\n",
    "    # show components\n",
    "    print(token, token.pos_, token.dep_) # token, part of speech (i.e. verb, noun), dependencies\n",
    "    \n",
    "# grab specific token\n",
    "#doc[3]\n",
    "\n",
    "# grab span (i.e. sub-section of doc)\n",
    "# this refers to index of token, not of char\n",
    "#span = doc[5:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-courtesy",
   "metadata": {},
   "source": [
    "### Sentences\n",
    "* You can split documents into sentences\n",
    "* You can also check various things such as whether or not a word is the start of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "young-ambassador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence.\n",
      "Is separate.\n",
      "To this sentence.\n"
     ]
    }
   ],
   "source": [
    "# load sentence into module\n",
    "doc = nlp(u\"This sentence. Is separate. To this sentence.\")\n",
    "\n",
    "# iterate through sentences\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "charitable-wallpaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if word is start of sentence\n",
    "print(doc[3])\n",
    "print(doc[3].is_sent_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-casting",
   "metadata": {},
   "source": [
    "### Pipeline:\n",
    "* Tokenization is just one of the steps within the NLP pipeline\n",
    "* You can see that Spacy runs multpiple different stages when parsing\n",
    "* We will cover the below stages in more detail later\n",
    "* You can add in custom pipeline stages if you want as well\n",
    "* These stages can either be simple rule-based steps or statistically trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "explicit-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1ba4de87be8>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1ba50ac8f48>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1ba50ad7278>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1ba50ad73c8>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1ba50bb2988>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1ba50bb2448>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show series of operations run when reading text into doc\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-asian",
   "metadata": {},
   "source": [
    "### Examples of Tokenization\n",
    "* Certain tokens are kept together for various reasons\n",
    "* Below, you can see that times, monetary values, emails etc. are kept together\n",
    "* Whilst certain forms of punctuation are split out specifically (e.g. !, ., \")\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accessory-awareness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "Send\n",
      "notes\n",
      "to\n",
      "snail-mail@hotmail.co.uk\n",
      "before\n",
      "9:00a.m\n",
      ".\n",
      "at\n",
      "a\n",
      "cost\n",
      "of\n",
      "$\n",
      "10.30\n"
     ]
    }
   ],
   "source": [
    "# store text in module\n",
    "doc = nlp(u'We\\'re here to help! Send notes to snail-mail@hotmail.co.uk before 9:00a.m. at a cost of $10.30')\n",
    "\n",
    "# show all tokens\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satisfactory-overall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length of document (i.e. token #)\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advance-cement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of lexemes (words) in the current library\n",
    "len(doc.vocab)\n",
    "\n",
    "# does the same thing as the above (library length doesn't change)\n",
    "# length is determined by the library you load (i.e. en_core_web_sm)\n",
    "# not the object you're reading\n",
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "democratic-tower",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.doc.Doc' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-58007d0db23e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# tokens cannot be reassigned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"new text\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# tokens cannot be reassigned\n",
    "doc[3] = \"new text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-velvet",
   "metadata": {},
   "source": [
    "### Named Entitites\n",
    "* Spacy can identify persons, companies, locations etc.\n",
    "* These are known as named entities\n",
    "* There are a number of default rules and entities loaded into the Spacy libraries\n",
    "* You can extract the entities specifically from the tokens (see below)\n",
    "* Spacy provides a number of attributes for entitites\n",
    "    * Label: tells you what the label is (e.g. organisation, money, country)\n",
    "    * Explain: describes exactly what the label means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "completed-running",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG Companies, agencies, institutions, etc.\n",
      "Hong Kong GPE Countries, cities, states\n",
      "$6 million MONEY Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# text containing named entities\n",
    "doc = nlp(u'Apple to build Hong Kong factory for $6 million.')\n",
    "\n",
    "# iterate through tokens and show entities\n",
    "for entity in doc.ents:\n",
    "    print(entity, entity.label_, str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-prayer",
   "metadata": {},
   "source": [
    "### Noun Chunks\n",
    "* Noun chunks can also be split out from text\n",
    "* Noun chunks are small sections of text where you have a noun with any attached descriptors\n",
    "* This helps you extract sections of meaning and can be used for things like sentiment analysis or general meaning of overall text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "blind-exchange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "an insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "# text with noun chunks\n",
    "doc = nlp(u'Autonomous cars are an insurance liability for manufacturers')\n",
    "\n",
    "# iterate through noun chunks\n",
    "for nc in doc.noun_chunks:\n",
    "    print(nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-plasma",
   "metadata": {},
   "source": [
    "### Visualizing Tokenization\n",
    "* You can actually visualize the tokenization steps Spacy is performing\n",
    "* **displacy** is the library to use for rendering this information\n",
    "* In the below example we've selected the dependency visualizer style\n",
    "* It shows you the labelled dependencies between each word and the part of speech label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "guided-resolution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"044f8a1fe2b54b809692492a9b0bde57-0\" class=\"displacy\" width=\"1150\" height=\"357.0\" direction=\"ltr\" style=\"max-width: none; height: 357.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">build</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">factory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">6</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">million.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-044f8a1fe2b54b809692492a9b0bde57-0-0\" stroke-width=\"2px\" d=\"M70,222.0 C70,112.0 260.0,112.0 260.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-044f8a1fe2b54b809692492a9b0bde57-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,224.0 L62,212.0 78,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-044f8a1fe2b54b809692492a9b0bde57-0-1\" stroke-width=\"2px\" d=\"M180,222.0 C180,167.0 255.0,167.0 255.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-044f8a1fe2b54b809692492a9b0bde57-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M180,224.0 L172,212.0 188,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-044f8a1fe2b54b809692492a9b0bde57-0-2\" stroke-width=\"2px\" d=\"M400,222.0 C400,112.0 590.0,112.0 590.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-044f8a1fe2b54b809692492a9b0bde57-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400,224.0 L392,212.0 408,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-044f8a1fe2b54b809692492a9b0bde57-0-3\" stroke-width=\"2px\" d=\"M510,222.0 C510,167.0 585.0,167.0 585.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-044f8a1fe2b54b809692492a9b0bde57-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M510,224.0 L502,212.0 518,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-044f8a1fe2b54b809692492a9b0bde57-0-4\" stroke-width=\"2px\" d=\"M290,222.0 C290,57.0 595.0,57.0 595.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-044f8a1fe2b54b809692492a9b0bde57-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595.0,224.0 L603.0,212.0 587.0,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-044f8a1fe2b54b809692492a9b0bde57-0-5\" stroke-width=\"2px\" d=\"M290,222.0 C290,2.0 710.0,2.0 710.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-044f8a1fe2b54b809692492a9b0bde57-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M710.0,224.0 L718.0,212.0 702.0,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-044f8a1fe2b54b809692492a9b0bde57-0-6\" stroke-width=\"2px\" d=\"M840,222.0 C840,112.0 1030.0,112.0 1030.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-044f8a1fe2b54b809692492a9b0bde57-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M840,224.0 L832,212.0 848,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-044f8a1fe2b54b809692492a9b0bde57-0-7\" stroke-width=\"2px\" d=\"M950,222.0 C950,167.0 1025.0,167.0 1025.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-044f8a1fe2b54b809692492a9b0bde57-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M950,224.0 L942,212.0 958,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-044f8a1fe2b54b809692492a9b0bde57-0-8\" stroke-width=\"2px\" d=\"M730,222.0 C730,57.0 1035.0,57.0 1035.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-044f8a1fe2b54b809692492a9b0bde57-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1035.0,224.0 L1043.0,212.0 1027.0,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load libraries\n",
    "from spacy import displacy\n",
    "\n",
    "# text\n",
    "doc = nlp(u'Apple to build a U.K. factory for $6 million.')\n",
    "\n",
    "# render the tokenization process\n",
    "# specify that it's being performed in Jupyter (rather than in a script etc.)\n",
    "# specify dependency style to display dependencies specifically\n",
    "# distance lets you control space between tokens\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance':110})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-december",
   "metadata": {},
   "source": [
    "* There are other views, such as the entity view\n",
    "* This highlights different entities within your text and colour code them for ease of interpretation\n",
    "* [Spacy Visualization Options](https://spacy.io/usage/visualizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ignored-resort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    almost 25,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a total profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $25 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# text\n",
    "doc = nlp(u'Over the last quarter, Apple sold almost 25,000 iPods for a total profit of $25 million.')\n",
    "\n",
    "# render using entity style\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text\n",
    "doc = nlp(u'This is a sentence.')\n",
    "\n",
    "# you can display your visualizations on a separate server\n",
    "# this can be useful if you're running a script external to Jupyter\n",
    "displacy.serve(doc, style='dep');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-horse",
   "metadata": {},
   "source": [
    "## 2. Stemming\n",
    "### Stemming Basics\n",
    "* Stemming is the process of reducing a word down to its stem (e.g. Caresses > Caress, Meeting > Meet)\n",
    "* It's quite a crude process as it basically just hacks bits off words to get to the root\n",
    "* This can result in quite a few errors or missclassifications\n",
    "* As such, **lemmatization** is the preferred technique (we'll look at this next) but stemming is useful to know for reference\n",
    "* Stemming essentially applies multiple stages of cropping (based on different rules) to get to the stem of the word\n",
    "* Porter's Algorithm is one of the most commonly used methods (5 stages of cropping) whilst Snowball is a slightly newer, more efficient method (a.k.a. Porter's 2)\n",
    "* We will use **NLTK** to implement stemming here as Spacy does not include stemming (it uses lemmatization instead)\n",
    "* [Why Spacy doesn't use stemming](https://github.com/explosion/spaCy/issues/327)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "passive-affair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ----> run\n",
      "runner ----> runner\n",
      "ran ----> ran\n",
      "runs ----> run\n",
      "easily ----> easili\n",
      "fairly ----> fairli\n",
      "fairness ----> fair\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# create stemming object\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# list of words\n",
    "words = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly', 'fairness'] # mix of verbs and nouns\n",
    "\n",
    "# show word and stem\n",
    "for word in words:\n",
    "    print(word + ' ----> ' + p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-current",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* You can see that the Porter's method above behaves differently to the Snowball method below\n",
    "* For both, runner is recognised as a noun, hence why it doesn't get cropped to e.g. run\n",
    "* Easily and fairly get cropped to have an i at the end of them in Porter's whilst Snowball is better at splitting fairly for example\n",
    "* Essentially, both methods have different sets of algorithmic rules and will work better/worse than one another in different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eight-republic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ----> run\n",
      "runner ----> runner\n",
      "ran ----> ran\n",
      "runs ----> run\n",
      "easily ----> easili\n",
      "fairly ----> fair\n",
      "fairness ----> fair\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# create stemming object (requires language)\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# stem words again\n",
    "for word in words:\n",
    "    print(word + ' ----> ' + s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-finish",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* The important thing here is not necessarily the exact root that is determined\n",
    "* The main thing is that words with the same root (in reality) are given the same exact root by the algorithm\n",
    "* For example, in Porter's method above, run and ran receive different roots whereas in the Snowball method they are given the same root\n",
    "* Neither of these is necessarily right or wrong, it depends on how you're looking to group similar words\n",
    "* So knowing the algorithmic steps implemented by your selected method is important to ensure you're grouping words that you want to be seen as having the same root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lonely-withdrawal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous ----> generous\n",
      "generously ----> generous\n",
      "generate ----> generat\n",
      "generation ----> generat\n"
     ]
    }
   ],
   "source": [
    "# new set of words\n",
    "words = ['generous', 'generously', 'generate', 'generation']\n",
    "\n",
    "# stem words again\n",
    "for word in words:\n",
    "    print(word + ' ----> ' + s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-uganda",
   "metadata": {},
   "source": [
    "## 3. Lemmatization\n",
    "### Lemmatization Basics\n",
    "* Lemmatization is a far more advanced method of getting to a word's root than stemming\n",
    "* Instead of simply cropping parts off a word, it has knowledge of different words and their roots (e.g. running > run)\n",
    "* It also looks at the context of a word (i.e. words before/after, parts of speech etc.) to determine a word's root\n",
    "* For example, it understands that meeting (noun) should remain as meeting, whilst meeting (verb) can be rooted to meet\n",
    "* Below, you can see that lemmatization breaks our text down in a specific way\n",
    "    * **Lemma** is the root object produced from lemmatization\n",
    "    * The **lemma_** object is the final root object\n",
    "    * Nouns retain their original text (i.e. runner > runner)\n",
    "    * Whilst verbs are rooted (i.e. running > run)\n",
    "    * Each lemma has its own unique hash value stored in **lemma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "opposed-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "am           AUX    10382539506755952630   be\n",
      "a            DET    11901859001352538922   a\n",
      "runner       NOUN   12640964157389618806   runner\n",
      "running      VERB   12767647472892411841   run\n",
      "in           ADP    3002984154512732771    in\n",
      "a            DET    11901859001352538922   a\n",
      "race         NOUN   8048469955494714898    race\n",
      "because      SCONJ  16950148841647037698   because\n",
      "I            PRON   4690420944186131903    I\n",
      "love         VERB   3702023516439754181    love\n",
      "to           PART   3791531372978436496    to\n",
      "run          VERB   12767647472892411841   run\n",
      "since        SCONJ  10066841407251338481   since\n",
      "I            PRON   4690420944186131903    I\n",
      "ran          VERB   12767647472892411841   run\n",
      "today        NOUN   11042482332948150395   today\n",
      ".            PUNCT  12646065887601541794   .\n"
     ]
    }
   ],
   "source": [
    "# text\n",
    "doc = nlp(u'I am a runner running in a race because I love to run since I ran today.')\n",
    "\n",
    "# method to format lemmas\n",
    "def show_lemmas(text):\n",
    "    # iterate through tokens\n",
    "    for token in text:\n",
    "        # show token text and lemma attributes\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')\n",
    "\n",
    "# show lemmas for text\n",
    "show_lemmas(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-dakota",
   "metadata": {},
   "source": [
    "## 4. Stop Words\n",
    "### Stop Words\n",
    "* Stop words are words like 'a', 'the' etc.\n",
    "* Essentially all the really common words which add little or nothing to the meaning of your text\n",
    "* They can harm your learning models because they add redundant noise to your text data\n",
    "* Spacy contains ~330 stop words in a **set** which you can access via the below method in order to filter them out of your text\n",
    "    * **NOTE:** sets are unordered, unindexed lists of items attached to a variable name\n",
    "    * They are 1 of 4 python collections (others being list, tuple and dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "native-exclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "{'part', 'than', 'cannot', 'ourselves', 'these', 'yours', 'besides', 'become', 'elsewhere', 'a', 'same', 'due', 'other', 'through', 'thereby', 'eleven', 'are', 'each', 'back', 'well', 'ca', 'be', 'himself', 'us', 'nowhere', 'between', 'mine', 'move', 'since', 'hence', 'few', 'beyond', 'an', 'behind', 'of', 'below', 'down', 'formerly', 'that', 'n’t', 'should', 'together', 'upon', 'became', 'five', 'toward', 'using', '‘re', 'most', 'whether', 'front', 'wherever', 'therein', 'has', 'six', 'everything', 'her', 'whereafter', 'onto', 'otherwise', 'however', 'least', '’m', 'they', 'ever', 'very', 'less', 'out', 'full', 'when', 'seem', 'nor', 'beside', 'twenty', 'its', 'whatever', 'doing', 'therefore', 'them', 'every', 'put', 'amount', 'after', 'never', 'further', 'though', 'i', 'it', \"'re\", 'am', 'seems', 'almost', 'although', 'he', 'noone', 'on', 'themselves', 'can', 'somewhere', 'take', 'either', 'others', 'amongst', 'hereupon', 'him', 'no', 'afterwards', 'yet', 'whole', 'the', 'moreover', 'whose', 'ours', '’s', 'this', 'how', 'everyone', 'again', 'alone', 'own', 'show', 'ten', 'me', 'via', 'nevertheless', 'across', 'fifteen', 'empty', 'eight', 'but', '‘d', 'various', 'side', 'first', 'three', 'then', 'as', 'next', 'whereby', 'give', 'somehow', 'around', 'former', 'something', 'too', 'get', 'from', 'else', 'may', 'could', 'whereas', 'always', 'herein', 'make', 'someone', 'often', 'was', 'fifty', 'really', 'you', '’re', 'now', 'until', 'indeed', 'for', 'will', 'becoming', 'latterly', 'bottom', 'rather', 'everywhere', 'our', 'n‘t', 'within', 'just', 'serious', 'several', 'thru', 'all', 'only', '‘ve', 'enough', 'been', 'we', '‘s', 'made', 'none', 'with', 'thence', 'by', 'sixty', 'whenever', 'being', 'your', 'some', \"n't\", 'why', 'perhaps', 'along', 'yourself', 'more', 'per', 'does', 'nine', 'towards', 'anywhere', 'anyway', 'latter', 'hereafter', 'off', 'myself', 'done', 'not', 'third', 'another', 'here', 'his', 'many', 'herself', 'still', 'those', 'name', 'hundred', 'above', 'thereafter', \"'ve\", 'go', 'anything', 'if', 'must', 'in', \"'ll\", 'whither', 'do', 'about', 'over', 'top', 'one', 'among', 'without', '‘m', 'seemed', 'both', 'please', 'would', 'except', 'to', 'up', 'into', 'used', 'yourselves', 'hers', 'is', 'sometime', 'hereby', 'their', 'because', 'much', 'twelve', '’d', '‘ll', \"'m\", 'under', 'mostly', 'thus', 'nobody', 'before', 'neither', 'who', 'might', 'forty', 'regarding', 'while', \"'s\", 'beforehand', 'anyhow', 'against', 'becomes', 'itself', 're', 'last', 'whereupon', 'meanwhile', 'so', 'already', 'also', 'had', 'sometimes', 'were', 'during', 'say', 'see', 'whence', 'nothing', 'whoever', 'whom', 'which', 'did', '’ll', 'my', 'once', 'have', 'throughout', '’ve', 'at', 'unless', 'two', 'namely', 'anyone', 'even', 'or', \"'d\", 'four', 'thereupon', 'where', 'and', 'such', 'keep', 'wherein', 'any', 'she', 'what', 'there', 'seeming', 'quite', 'call'}\n"
     ]
    }
   ],
   "source": [
    "# show all stop words\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "still-haven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if specific word is a stop word\n",
    "# the vocab object lets you perform a number of operations on text\n",
    "nlp.vocab['is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "laden-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add specific word to list of stop words\n",
    "# both lines are required to make it a stop word\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "\n",
    "# remove words from stop words\n",
    "nlp.Defaults.stop_words.remove('btw')\n",
    "nlp.vocab['btw'].is_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-matter",
   "metadata": {},
   "source": [
    "## 5. Phrase Matching & Vocab\n",
    "### Rule-Based Matching\n",
    "* Phrase matching can be thought of as an advanced form of regular expressions\n",
    "* Again, you are looking for patterns to match and extract from text\n",
    "* However, this time you are looking at specific Spacy text elements (e.g. parts of speech) to help you more accurately match sections of text\n",
    "* You create a **matcher** object to run phrase matching\n",
    "* You define specific patterns (see tables further down for quantifiers and attributes) to match on\n",
    "* Each pattern is a list of dictionaries, where each dictionary references one token\n",
    "* [Spacy rule-based matching](https://spacy.io/usage/rule-based-matching)\n",
    "\n",
    "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
    "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
    "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
    "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
    "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
    "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
    "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
    "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
    "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
    "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "inclusive-third",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 8 11 solar-power\n",
      "8656102463236116519 SolarPower 13 14 SolarPower\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# create matcher object (based on loaded language library)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# define pattern to match\n",
    "# trying to match all of the below cases\n",
    "# SolarPower, Solar-power, Solar power\n",
    "pattern1 = [{'LOWER':'solarpower'}] # for all one word, any case (e.g. SolarPower, SOLARPOWER)\n",
    "pattern2 = [{'LOWER':'solar'}, {'IS_PUNCT':True}, {'LOWER':'power'}] # match hyphenated word (e.g. Solar-power)\n",
    "pattern3 = [{'LOWER':'solar'}, {'LOWER':'power'}] # match two separate words, any case (e.g. solar power)\n",
    "\n",
    "# store all patterns in a list\n",
    "patterns = [pattern1, pattern2, pattern3]\n",
    "\n",
    "# alternate shorthand for writing above\n",
    "#patterns = [\n",
    "#    [{'LOWER':'solarpower'}],\n",
    "#    [{'LOWER':'solar'}, {'IS_PUNCT':True}, {'LOWER':'power'}],\n",
    "#    [{'LOWER':'solar'}, {'LOWER':'power'}]\n",
    "#]\n",
    "\n",
    "# add patterns to matcher object\n",
    "# name the specific matcher so you can access it\n",
    "# add any and all patterns you'd like to match within this matcher\n",
    "matcher.add('SolarPower', patterns)\n",
    "\n",
    "# create text\n",
    "doc = nlp(u'The Solar Power industry continues to grow as solar-power increases. SolarPower is awesome!')\n",
    "\n",
    "# find matches\n",
    "found_matches = matcher(doc)\n",
    "\n",
    "\n",
    "# method to print out match strings\n",
    "def show_matches(matches):\n",
    "    for match_id, start, stop in found_matches:\n",
    "        # get string using match_id\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "\n",
    "        # extract matched span using start and stop\n",
    "        span = doc[start:stop]\n",
    "\n",
    "        # show matched values\n",
    "        print(match_id, string_id, start, stop, span.text)\n",
    "        \n",
    "# show matches (shows unique ID for match, start and stop token index of match)\n",
    "show_matches(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-bible",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Your matcher is assigned an ID\n",
    "* Within it are all the matchers versions you've created (e.g. SolarPower above)\n",
    "* You can access the information attached to these objects using the IDs generated above\n",
    "* This is very useful for extracting specific matches from your matcher objects\n",
    "\n",
    "### Operators/Quantifiers\n",
    "* Just like regex, you can use additional **operators/quantifiers** to enhance your pattern matching\n",
    "* In the below code we ask our matcher to identify patterns where the words 'solar' and 'power' are separated by any number of punctuation\n",
    "* The below table shows other options for operators\n",
    "\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "occupied-manchester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 0 3 Solar--Power\n",
      "8656102463236116519 SolarPower 4 5 solarpower\n"
     ]
    }
   ],
   "source": [
    "# remove specific matcher pattern(s)\n",
    "matcher.remove('SolarPower')\n",
    "\n",
    "# create new patterns\n",
    "patterns = [\n",
    "    [{'LOWER':'solarpower'}],\n",
    "    [{'LOWER':'solar'}, {'IS_PUNCT':True, 'OP':'*'}, {'LOWER':'power'}] # operators/quantifiers to capture any amount of punctuation (see below)\n",
    "]\n",
    "\n",
    "# add patterns to matcher\n",
    "matcher.add('SolarPower', patterns)\n",
    "\n",
    "# create text\n",
    "doc = nlp(u'Solar--Power is solarpower yeah!')\n",
    "\n",
    "# match patterns in text\n",
    "found_matches = matcher(doc)\n",
    "\n",
    "# show matches\n",
    "show_matches(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-textbook",
   "metadata": {},
   "source": [
    "### Phrase Matching\n",
    "* Besides defining rules for pattern matching, you can look for specific phrases too\n",
    "* This might be helpful if you're trying to extract a subset of information based on specific criteria\n",
    "* The matcher object here works in much the same way as the rule based matcher above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "sunset-retention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680293220734633682 EconMatcher 41 45 supply-side economics\n",
      "3680293220734633682 EconMatcher 49 53 trickle-down economics\n",
      "3680293220734633682 EconMatcher 54 56 voodoo economics\n",
      "3680293220734633682 EconMatcher 61 65 free-market economics\n",
      "3680293220734633682 EconMatcher 673 677 supply-side economics\n",
      "3680293220734633682 EconMatcher 2987 2991 trickle-down economics\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# create matcher object\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# load text file\n",
    "with open('NLP Course Files/TextFiles/reaganomics.txt') as f:\n",
    "    doc = nlp(f.read())\n",
    "    \n",
    "# create list of phrases to search for\n",
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n",
    "\n",
    "# process each phrase into spacy\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "\n",
    "# load phrases into matcher\n",
    "matcher.add('EconMatcher', phrase_patterns)\n",
    "\n",
    "# get matches\n",
    "found_matches = matcher(doc)\n",
    "\n",
    "# show matches\n",
    "show_matches(found_matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
