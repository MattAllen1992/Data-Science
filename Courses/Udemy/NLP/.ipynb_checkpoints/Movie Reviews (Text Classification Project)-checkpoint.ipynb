{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accepting-budapest",
   "metadata": {},
   "source": [
    "# Movie Reviews (Text Classification Project)\n",
    "## 1. Prepare Data\n",
    "### Read in Data\n",
    "* Drop all nulls\n",
    "* Remove any blank reviews (i.e. whitespace only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pressing-agent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>how do films like mouse hunt get into theatres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>some talented actresses are blessed with a dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>this has been an extraordinary year for austra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>according to hollywood movies made in last few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>my first press screening of 1998 and already i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   neg  how do films like mouse hunt get into theatres...\n",
       "1   neg  some talented actresses are blessed with a dem...\n",
       "2   pos  this has been an extraordinary year for austra...\n",
       "3   pos  according to hollywood movies made in last few...\n",
       "4   neg  my first press screening of 1998 and already i..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in dataset\n",
    "in_path = 'C:/Users/Matthew.Allen2/Documents/GitHub/Data-Science-Private/Udemy/NLP Course Files/TextFiles/'\n",
    "df = pd.read_csv(in_path + 'moviereviews.tsv', sep='\\t')\n",
    "\n",
    "# peek at data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "express-premium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reliable-colleague",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "review    35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "overall-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nulls\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "amino-advocacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1938, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for blanks\n",
    "blanks = []\n",
    "\n",
    "# iterate through (index, label, review)\n",
    "for i, lb, rv in df.itertuples():\n",
    "    # check if whitespace\n",
    "    if rv.isspace():\n",
    "        # store index of blanks\n",
    "        blanks.append(i)\n",
    "        \n",
    "# remove blanks at selected index\n",
    "df.drop(blanks, inplace=True)\n",
    "\n",
    "# check output shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-proof",
   "metadata": {},
   "source": [
    "### Train, Test, Split Data\n",
    "* Prepare data for ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "meaning-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract X and y\n",
    "X, y = df['review'], df['label']\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-eight",
   "metadata": {},
   "source": [
    "## 2. Modelling\n",
    "### Build Pipeline\n",
    "* Stages for:\n",
    "    * Count Vectorize (frequency counts of each unique word)\n",
    "    * TF-IDF Transformation (importance of words relevant to their frequency across all documents)\n",
    "    * LinearSVC model (linear classifier based on maximising distance between support vectors and boundary)\n",
    "* Fit pipeline to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adverse-poker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# create pipeline object\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC())])\n",
    "\n",
    "# fit pipeline to training data\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-quantity",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "* Make predictions based off trained model\n",
    "* Assess accuracy (precision, recall, f1 etc.) of predicted values against known values on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "available-denmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>235</td>\n",
       "      <td>47</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>41</td>\n",
       "      <td>259</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>276</td>\n",
       "      <td>306</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  neg  pos  All\n",
       "Actual                  \n",
       "neg        235   47  282\n",
       "pos         41  259  300\n",
       "All        276  306  582"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# make test predictions\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# evaluate model\n",
    "# confusion matrix (use crosstab instead to accurately infer variable order)\n",
    "pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "familiar-snapshot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.83      0.84       282\n",
      "         pos       0.85      0.86      0.85       300\n",
      "\n",
      "    accuracy                           0.85       582\n",
      "   macro avg       0.85      0.85      0.85       582\n",
      "weighted avg       0.85      0.85      0.85       582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "first-scale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "print(round(accuracy_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-vertex",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* Our initial model isn't too bad\n",
    "* Overall accuracy is 85%\n",
    "* Precision, recall and f1-scores are consistent between positive and negative reviews\n",
    "* Therefore it's not struggling with one particular class, it's just a reasonable good predictor of both\n",
    "* Still a reasonable amount of missclassification though (evenly split between both classes i.e. FN and FP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
