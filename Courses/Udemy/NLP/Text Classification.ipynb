{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parental-advantage",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "## 1. SKLearn Classification\n",
    "### Classification\n",
    "* This is a supervised model whereby we have a number of features as well as the known labels\n",
    "* Here, we are trying to identify real (ham) vs. fake (spam) text messages\n",
    "* The features are the message text itself as well as the number of words and punctuation marks used\n",
    "* The labels are the known values of whether a text is ham or spam\n",
    "* SKLearn lets us use a number of ML models to predict whether a text is ham or spam based on its features\n",
    "\n",
    "### Basic Features (Length and Punct)\n",
    "* We will run a variety of models using just the length and punct features to predict ham/spam labels\n",
    "* We can compare the results of multiple models to see how well they classify each outcome\n",
    "* The models we'll use will mostly use default settings (rather than being optimized) just to give us an idea of how effective they will be\n",
    "* Initial data investigation shows us that there are no null values (so no requirement to handle missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "supported-lexington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      9\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      6\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n",
       "3   ham  U dun say so early hor... U c already then say...      49      6\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read text into df\n",
    "df = pd.read_csv('NLP Course Files/TextFiles/smsspamcollection.tsv', sep='\\t')\n",
    "\n",
    "# peek at data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chicken-wagner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      " 2   length   5572 non-null   int64 \n",
      " 3   punct    5572 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 174.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# df info (no nulls, length, column types)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brutal-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check label vals\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brutal-occasions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check label counts\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-colors",
   "metadata": {},
   "source": [
    "### Feature Distribution\n",
    "Length:\n",
    "* We can see from the below plot that ham and spam texts have a significantly different distribution of length\n",
    "* Spam texts tend to be longer than ham texts\n",
    "* Spam texts also have a much narrower range of lengths\n",
    "* This makes sense as genuine texts will vary depending on nature and are also likely to contain lots of short, convenience texts (e.g. \"See you later\", \"No worries\" etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "terminal-wheel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot distribution of ham/spam by length\n",
    "plt.xscale('log') # log scale due to large value range\n",
    "bins = 1.15**(np.arange(0,50))\n",
    "plt.hist(df[df['label'] == 'ham']['length'], bins=bins, alpha=0.8)\n",
    "plt.hist(df[df['label'] == 'spam']['length'], bins=bins, alpha=0.8)\n",
    "plt.legend(('ham', 'spam'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-watershed",
   "metadata": {},
   "source": [
    "Punctuation:\n",
    "* There is less of a clear distinction between ham and spam texts when looking at punctuation\n",
    "* There is a slight tendency for spam texts to have more punctuation than ham\n",
    "* However this is only a minor trend, nothing as stark as seen in length above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "joint-herald",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot distribution of ham/spam by punct\n",
    "plt.xscale('log') # log scale due to large value range\n",
    "bins = 1.5**(np.arange(0,15))\n",
    "plt.hist(df[df['label'] == 'ham']['punct'], bins=bins, alpha=0.8)\n",
    "plt.hist(df[df['label'] == 'spam']['punct'], bins=bins, alpha=0.8)\n",
    "plt.legend(('ham', 'spam'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-notion",
   "metadata": {},
   "source": [
    "### SKLearn Models\n",
    "* We will use a range of models (logistic regression, naive bayes and SVM) for an initial look at classification\n",
    "* We'll split our data into test and train sets before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "third-wagner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3900, 2), (1672, 2), (3900,), (1672,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# isolate features (X) and labels (y)\n",
    "X = df[['length', 'punct']]\n",
    "y = df['label']\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# check shape\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-elder",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "* All 3 of our models performed quite poorly against the ham/spam classification problem\n",
    "* Accuracy was not horrific (high 80%s) but all other metrics (precision, recall, f1) were very bad\n",
    "* The models were OK at correctly identifying ham texts, but failed almost entirely at identifying spam\n",
    "\n",
    "Confusion Matrix:\n",
    "* These allow you to compare the number of correct positive and negative results from your models\n",
    "* Accuracy = correct / total\n",
    "    * Accuracy is a useful metric if classes are imbalaced\n",
    "    * But a poor one if they're not\n",
    "    * e.g. if the real world occurrence of negative results is 1% then a model accuracy of 98% isn't good enough and will miss the true negatives\n",
    "    * As such, you must use other metrics (precision, recall, f1 etc.)\n",
    "* Precision = TP / (TP + FP)\n",
    "    * Essentially tells you how many of your true predictions were relevant\n",
    "* Recall = TP / (TP + FN)\n",
    "    * Essentially tells you how many true predictions you made compared to all actual true values\n",
    "* F1-score is a harmonic mean of precision and recall\n",
    "    * A harmonic mean balances the means of the input variable's means\n",
    "    * The benefit of this is that it penalises extreme errors\n",
    "    * e.g. if your recall mean is 0 and your precision mean is 1, your f1-score will also be 0 rather than 0.5 (i.e. correctly indicates that your model is awful in one area at least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "statutory-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "       ham  spam\n",
      "ham   1404    44\n",
      "spam   219     5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.87      0.97      0.91      1448\n",
      "        spam       0.10      0.02      0.04       224\n",
      "\n",
      "    accuracy                           0.84      1672\n",
      "   macro avg       0.48      0.50      0.48      1672\n",
      "weighted avg       0.76      0.84      0.80      1672\n",
      "\n",
      "0.84\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "       ham  spam\n",
      "ham   1438    10\n",
      "spam   224     0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.87      0.99      0.92      1448\n",
      "        spam       0.00      0.00      0.00       224\n",
      "\n",
      "    accuracy                           0.86      1672\n",
      "   macro avg       0.43      0.50      0.46      1672\n",
      "weighted avg       0.75      0.86      0.80      1672\n",
      "\n",
      "0.86\n",
      "\n",
      "\n",
      "SVM\n",
      "       ham  spam\n",
      "ham   1373    75\n",
      "spam   121   103\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.92      0.95      0.93      1448\n",
      "        spam       0.58      0.46      0.51       224\n",
      "\n",
      "    accuracy                           0.88      1672\n",
      "   macro avg       0.75      0.70      0.72      1672\n",
      "weighted avg       0.87      0.88      0.88      1672\n",
      "\n",
      "0.88\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "# create model instance\n",
    "lr_model = LogisticRegression(solver='lbfgs')\n",
    "nb_model = MultinomialNB()\n",
    "svc_model = SVC(gamma='auto')\n",
    "\n",
    "# store models in list\n",
    "models = [[lr_model, 'Logistic Regression'],\n",
    "          [nb_model, 'Naive Bayes'],\n",
    "          [svc_model, 'SVM']]\n",
    "\n",
    "# iterate through models and run fit, predict and evaluation\n",
    "for model, model_name in models:\n",
    "    # print model name\n",
    "    print(model_name)\n",
    "    \n",
    "    # fit model to data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # confusion matrix (act vs. pred)\n",
    "    print(pd.DataFrame(metrics.confusion_matrix(y_test, y_pred), index=['ham', 'spam'], columns=['ham', 'spam']))\n",
    "\n",
    "    # classification report\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "    # model overall accuracy\n",
    "    print(round(metrics.accuracy_score(y_test, y_pred),2))\n",
    "    \n",
    "    # newline separator\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-moses",
   "metadata": {},
   "source": [
    "## 2. Text Feature Extraction\n",
    "### Extracting Numerical Meaning from Raw Text (Unstructured Data)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from sklearn.feature_extraction import TfidfVectorizer\n",
    "\n",
    "# create vectorizer instance\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "# create document term matrix\n",
    "dtm = vect.fit_transform(list_of_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-friendship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-bulletin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-thirty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-penetration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-craps",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-question",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-chaos",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-shield",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-desperate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-cardiff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-yield",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-fairy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-sailing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-malaysia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-president",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-drove",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-break",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-flashing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-medline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-thing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-north",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-armstrong",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-fight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-moore",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
