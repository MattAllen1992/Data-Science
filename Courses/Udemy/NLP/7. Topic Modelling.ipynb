{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lightweight-generic",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "## 1. Overview\n",
    "* Topic modelling is the process of identifying groups of similar words which occur together in a document that can allow you to infer a specific topic\n",
    "* For example, if the words 'dog', 'cat', 'vet' etc. occurred commonly in a document, you could say that a suitable topic for that document is 'pets'\n",
    "* Topics can be thought of mathematically as a distribution of words, where certain words occur far more frequently than others and give meaning to the topic selected\n",
    "* Documents can be thought of as distributions of topics (where each document contains more than one topic) and you can assign a specific topic to a document based on the frequently occuring topic/set of words\n",
    "* Topic modelling is an unsupervised method, where the user must determine the number of topics to be extracted and the labels to be assigned to each topic (this process is never perfect and purely depends on user preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stock-lodging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "\n",
    "# load text data\n",
    "npr = pd.read_csv('NLP Course Files/TextFiles/npr.csv')\n",
    "\n",
    "# peek at data\n",
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "working-straight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7646"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine first article length\n",
    "len(npr['Article'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scenic-mustang",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11992, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine df\n",
    "npr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-interface",
   "metadata": {},
   "source": [
    "## 2. Latent Dirichlet Allocation\n",
    "* There are many options for topic modelling, but a well tested one in Python is LDA (based on the Dirichlet distribution)\n",
    "* The process for topic modelling with LDA is as follows:\n",
    "    * User picks n_topics to extract from the data\n",
    "    * LDA algorithm calculates the probability of a topic in a document\n",
    "    * It also calculates the probability of a word in a topic\n",
    "    * It then iteratively (many, many times!) re-assigns new, more accurate topics to each word based on the proportion of words throughout all documents that have been assigned to each topic\n",
    "    * This essentially hones in on the best topic to assign to each word, and therefore the best topic to assign to each document based on frequency and importance of words across all docs\n",
    "    * The user then analyses the most frequent words within the selected topic of a document and labels the topic accordingly\n",
    "* The text is processed by the LDA algorithm by first converting words to vectors to allow numerical computation of the document term matrix (DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exterior-pollution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11992x54777 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3033388 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer # get word/token frequencies\n",
    "\n",
    "# create vectorizer instance\n",
    "# max_df = discard words that show up in 90% of docs (e.g. stop words, really common words)\n",
    "# min_df = discard words that occur in < 2 docs (e.g. typos, mis-spelling)\n",
    "# remove all stop words (e.g. the, is, a...)\n",
    "cv = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "\n",
    "# fit model to data and transform into vectors\n",
    "# dtm = document term matrix (docs x words)\n",
    "dtm = cv.fit_transform(npr['Article'])\n",
    "\n",
    "# check output (should be sparse matrix)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "structural-citation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# create object instance\n",
    "# no right or wrong for n_components (i.e. topics), you can experiment here\n",
    "lda = LatentDirichletAllocation(n_components=7, random_state=42)\n",
    "\n",
    "# fit object to data (can take a while as it's an iterative process)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-award",
   "metadata": {},
   "source": [
    "### Extracting Latent Topics\n",
    "* They are known as latent topics because the LDA algorithm hasn't defined labels or descriptions of these topics, it's simply inferred similarity between groups of words\n",
    "* We can access the groups of words attached to each topic via the **get_feature_names()** method\n",
    "* Here, each word has been assigned a probability of falling into each of the defined topics\n",
    "* We can sort by word probability within specific topics to see the most important words to that topic and begin to define the topic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reported-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "54777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'psychoactive'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizer feature names is just a list of words across all our docs\n",
    "print(type(cv.get_feature_names()))\n",
    "\n",
    "# check size of vectorizer\n",
    "# length should be identical to # unique words in our dtm (above)\n",
    "# this is because our vectorizer has one vector per unique word\n",
    "print(len(cv.get_feature_names()))\n",
    "\n",
    "# we can therefore get words directly from it\n",
    "cv.get_feature_names()[38755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opening-survival",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "<class 'numpy.ndarray'>\n",
      "(7, 54777)\n",
      "\n",
      "\n",
      "TOP 15 WORDS FOR TOPIC #0\n",
      "['companies', 'money', 'year', 'federal', '000', 'new', 'percent', 'government', 'company', 'million', 'care', 'people', 'health', 'said', 'says']\n",
      "\n",
      "\n",
      "TOP 15 WORDS FOR TOPIC #1\n",
      "['military', 'house', 'security', 'russia', 'government', 'npr', 'reports', 'says', 'news', 'people', 'told', 'police', 'president', 'trump', 'said']\n",
      "\n",
      "\n",
      "TOP 15 WORDS FOR TOPIC #2\n",
      "['way', 'world', 'family', 'home', 'day', 'time', 'water', 'city', 'new', 'years', 'food', 'just', 'people', 'like', 'says']\n",
      "\n",
      "\n",
      "TOP 15 WORDS FOR TOPIC #3\n",
      "['time', 'new', 'don', 'years', 'medical', 'disease', 'patients', 'just', 'children', 'study', 'like', 'women', 'health', 'people', 'says']\n",
      "\n",
      "\n",
      "TOP 15 WORDS FOR TOPIC #4\n",
      "['voters', 'vote', 'election', 'party', 'new', 'obama', 'court', 'republican', 'campaign', 'people', 'state', 'president', 'clinton', 'said', 'trump']\n",
      "\n",
      "\n",
      "TOP 15 WORDS FOR TOPIC #5\n",
      "['years', 'going', 've', 'life', 'don', 'new', 'way', 'music', 'really', 'time', 'know', 'think', 'people', 'just', 'like']\n",
      "\n",
      "\n",
      "TOP 15 WORDS FOR TOPIC #6\n",
      "['student', 'years', 'data', 'science', 'university', 'people', 'time', 'schools', 'just', 'education', 'new', 'like', 'students', 'school', 'says']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# investigate topics\n",
    "print(len(lda.components_)) # number of topics\n",
    "print(type(lda.components_)) # ndarray of 7 topic probabilities per word\n",
    "print(lda.components_.shape) # shape is # topics x # words\n",
    "print('\\n')\n",
    "\n",
    "# iterate through topics, extracting top 15 words per topic\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f'TOP 15 WORDS FOR TOPIC #{i}')\n",
    "    \n",
    "    # return index positions sorted from least to greatest value\n",
    "    # get top 15 values (i.e. highest probability words from topic)\n",
    "    print([cv.get_feature_names()[index] for index in topic.argsort()[-15:]])    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-harbor",
   "metadata": {},
   "source": [
    "### Assigning Topics\n",
    "* Once we have created our LDA model for determining topic from words mix, we can assign the highest probability topic to each of the articles/documents in our input data\n",
    "* We can use our LDA model to transform our DTM vector values into probabilities that show us which topic each word is most relevant to\n",
    "* If you've selected n_topics, you will have n_probabilities per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hungry-chrome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.68, 0.  , 0.  , 0.3 , 0.  , 0.  ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate probabilities of topics per document/article\n",
    "topic_results = lda.transform(dtm)\n",
    "\n",
    "# check first result (% probability of word per topic)\n",
    "topic_results[0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aging-hearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic\n",
       "0  In the Washington of 2016, even when the polic...      1\n",
       "1    Donald Trump has used Twitter  —   his prefe...      1\n",
       "2    Donald Trump is unabashedly praising Russian...      1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n",
       "4  From photography, illustration and video, to d...      2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign topics to each article/document\n",
    "# argmax() = get index position of highest probability\n",
    "npr['Topic'] = topic_results.argmax(axis=1)\n",
    "\n",
    "# check new data\n",
    "npr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-perth",
   "metadata": {},
   "source": [
    "## 3. Non-Negative Matrix Factorization\n",
    "* NMF is an alternative method to LDA\n",
    "* Both allow you to perform topic modelling but utilizing different methods\n",
    "* LDA is a probabilistic method, calculating the likelihood of words occurring within topics and optimizing topic and document topic assignment via iterative reassignment\n",
    "* NMF is a linear algebraic method which performs dimensionality reduction and clustering simultaneously (similar to PCA) in order to determine topic assignment and coefficients for words and documents\n",
    "* Both have their pros and cons, we will now look at NMF to contrast it to the earlier LDA code\n",
    "* [LDA vs. NMF](https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "laden-applicant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11992x54777 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3033388 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# load text data\n",
    "npr = pd.read_csv('NLP Course Files/TextFiles/npr.csv')\n",
    "\n",
    "# create vectorizer instance\n",
    "# filter out words that occur in >95% of docs and < 2 occurrences total\n",
    "# remove stop words to prevent noise\n",
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "# create dtm using vectorizer\n",
    "dtm = tfidf.fit_transform(npr['Article'])\n",
    "\n",
    "# check dtm\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-dietary",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* The first step is to convert our raw text into a TF-IDF vectorized sparse matrix (document term matrix - DTM)\n",
    "* During this process, stop words are removed, raw text is converted into numerical vectors and weighting is applied to handle the frequency of word occurrence across all documents (TF-IDF)\n",
    "* This DTM is then our non-negative matrix input for the NMF stage\n",
    "* By specifying the number of topics we would like to determine, the NMF model splits our DTM into a clustered matrix (words by topic) and a coefficient matrix (documents by topic) where all values are coefficients (not probabilities)\n",
    "\n",
    "**NOTE:**\n",
    "   * Fitting NMF to the data will likely be quicker than LDA because numpy is very well suited to the transformation algorithms implemented by NMF\n",
    "   * In general, NMF is much faster than LDA, particularly for very large datasets\n",
    "   * The topics returned by NFM will not be identical to LDA, even on the same dataset. They might be similar, but because the algorithms are different, so too will be the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fresh-external",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC # 0\n",
      "['new', 'research', 'like', 'patients', 'health', 'disease', 'percent', 'women', 'virus', 'study', 'water', 'food', 'people', 'zika', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 1\n",
      "['gop', 'pence', 'presidential', 'russia', 'administration', 'election', 'republican', 'obama', 'white', 'house', 'donald', 'campaign', 'said', 'president', 'trump']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 2\n",
      "['senate', 'house', 'people', 'act', 'law', 'tax', 'plan', 'republicans', 'affordable', 'obamacare', 'coverage', 'medicaid', 'insurance', 'care', 'health']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 3\n",
      "['officers', 'syria', 'security', 'department', 'law', 'isis', 'russia', 'government', 'state', 'attack', 'president', 'reports', 'court', 'said', 'police']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 4\n",
      "['primary', 'cruz', 'election', 'democrats', 'percent', 'party', 'delegates', 'vote', 'state', 'democratic', 'hillary', 'campaign', 'voters', 'sanders', 'clinton']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 5\n",
      "['love', 've', 'don', 'album', 'way', 'time', 'song', 'life', 'really', 'know', 'people', 'think', 'just', 'music', 'like']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 6\n",
      "['teacher', 'state', 'high', 'says', 'parents', 'devos', 'children', 'college', 'kids', 'teachers', 'student', 'education', 'schools', 'school', 'students']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# create model instance (aim for 7 topics)\n",
    "nmf_model = NMF(n_components=7, random_state=42)\n",
    "\n",
    "# fit model to dtm\n",
    "nmf_model.fit(dtm)\n",
    "\n",
    "# investigate topics by words\n",
    "for index, topic in enumerate(nmf_model.components_):\n",
    "    # get top 15 words per topic\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC # {index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "protected-cheese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Topic Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "      <td>Election 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "      <td>Election 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "      <td>Election 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>3</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>6</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic Topic Label\n",
       "0  In the Washington of 2016, even when the polic...      1  Election 1\n",
       "1    Donald Trump has used Twitter  —   his prefe...      1  Election 1\n",
       "2    Donald Trump is unabashedly praising Russian...      1  Election 1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      3    Politics\n",
       "4  From photography, illustration and video, to d...      6   Education"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate topic label per document and word in dtm\n",
    "# each result in this object is the probability/coefficient for each topic (per document)\n",
    "topic_results = nmf_model.transform(dtm)\n",
    "\n",
    "# assign highest value topic label for each document\n",
    "npr['Topic'] = topic_results.argmax(axis=1)\n",
    "\n",
    "# create topic labels dict\n",
    "topic_dict = {0:'Health', 1:'Election 1', 2:'Healthcare', 3:'Politics', 4:'Election 2', 5:'Music', 6:'Education'}\n",
    "\n",
    "# assign labels to topics in df\n",
    "npr['Topic Label'] = npr['Topic'].map(topic_dict)\n",
    "\n",
    "# check output\n",
    "npr.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
